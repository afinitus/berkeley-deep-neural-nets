{"cells":[{"cell_type":"markdown","metadata":{"id":"zcav88u_I45M"},"source":["# Prompting Language Models\n","\n","In ths notebook, we'll be evaluating different model prompting strategies on a publicly available language model. We will then perform soft-prompt tuning on GPT-2 and compare it against hard prompting."]},{"cell_type":"markdown","metadata":{"id":"ShJPg2nFI45N"},"source":["## Install required packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Cs5P34rU7rT"},"outputs":[],"source":["!pip install transformers\n","!pip install inflect\n","!pip install cohere"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UCd-lD2jU7rX"},"outputs":[],"source":["import pickle as pkl\n","import os\n","import json\n","import cohere\n","import random\n","import inflect\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pickle as pkl\n","import torch\n","from torch import nn\n","import time\n","from collections import deque\n","\n","random.seed(0)\n","np.random.seed(0)"]},{"cell_type":"markdown","metadata":{"id":"OnghhJjYU7rY"},"source":["# Load the dataset\n","\n","We will be using the Common Sense QA dataset, which is a collection of questions about everyday life. The cells below download the data from https://www.tau-nlp.sites.tau.ac.il/commonsenseqa ."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N9rldi5WU7rZ"},"outputs":[],"source":["# Load dataset from jsonl file\n","def make_dataset(path):\n","    dataset = []\n","    with open(path) as f:\n","        for line in f:\n","            dataset.append(json.loads(line))\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FLwUUEk3U7rZ"},"outputs":[],"source":["#@title download the dataset\n","!curl https://s3.amazonaws.com/commensenseqa/train_rand_split.jsonl -o train_rand_split.jsonl\n","!curl https://s3.amazonaws.com/commensenseqa/dev_rand_split.jsonl -o dev_rand_split.jsonl\n","!curl https://s3.amazonaws.com/commensenseqa/test_rand_split_no_answers.jsonl -o test_rand_split_no_answers.jsonl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YhoOwcDYU7ra"},"outputs":[],"source":["train_set = make_dataset('train_rand_split.jsonl')\n","val_set = make_dataset('dev_rand_split.jsonl')\n","\n","# Print the lengths of the train and validation sets\n","print(len(train_set), len(val_set))"]},{"cell_type":"markdown","metadata":{"id":"p02tkKfGU7ra"},"source":["Here are a few examples of the dataset format:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"se1rOuGkU7rb"},"outputs":[],"source":["val_set[:3]"]},{"cell_type":"markdown","metadata":{"id":"3fUcl1PcU7rc"},"source":["Make an Cohere account, generate a **trial** API key at https://dashboard.cohere.ai/api-keys, and paste it below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w258J--PU7rc"},"outputs":[],"source":["# Set the api key from https://dashboard.cohere.ai/api-keys\n","co = cohere.Client('<YOUR API KEY>')"]},{"cell_type":"markdown","metadata":{"id":"bWXXcazzU7rc"},"source":["In this notebook, we'll explore different hard-prompting strategies. Run the cells below to see a few example strategies."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2caC6yf4U7rd"},"outputs":[],"source":["def make_simple_prompt(data_point):\n","    prompt = f\"\"\"{data_point['question']['stem']}\n"," {data_point['question']['choices'][0]['label']} {data_point['question']['choices'][0]['text']}\n"," {data_point['question']['choices'][1]['label']} {data_point['question']['choices'][1]['text']}\n"," {data_point['question']['choices'][2]['label']} {data_point['question']['choices'][2]['text']}\n"," {data_point['question']['choices'][3]['label']} {data_point['question']['choices'][3]['text']}\n"," {data_point['question']['choices'][4]['label']} {data_point['question']['choices'][4]['text']}\n","\"\"\"\n","    return prompt\n","\n","def make_simple_qa_prompt(data_point):\n","    prompt = f\"\"\"Question: {data_point['question']['stem']}\n","Choice {data_point['question']['choices'][0]['label']}: {data_point['question']['choices'][0]['text']}\n","Choice {data_point['question']['choices'][1]['label']}: {data_point['question']['choices'][1]['text']}\n","Choice {data_point['question']['choices'][2]['label']}: {data_point['question']['choices'][2]['text']}\n","Choice {data_point['question']['choices'][3]['label']}: {data_point['question']['choices'][3]['text']}\n","Choice {data_point['question']['choices'][4]['label']}: {data_point['question']['choices'][4]['text']}\n","Answer:\"\"\"\n","    return prompt\n","\n","def get_instruction():\n","    return \"Answer the following question with A, B, C, D, or E.\\n\"\n","\n","def make_qa_instruction_prompt(data_point):\n","    prompt = get_instruction()\n","    prompt += make_simple_qa_prompt(data_point)\n","    return prompt\n","\n","def make_few_shot_prompt(data_point, num_shots):\n","    prompt = get_instruction()\n","    for i in range(num_shots):\n","        prompt += make_simple_qa_prompt(train_set[i])\n","        prompt += f\" {train_set[i]['answerKey']}\\n\"\n","    prompt += make_simple_qa_prompt(data_point)\n","    return prompt\n","\n","# This is like the prompt above, but the answers in the examples given are random, not the correct answer\n","def make_incorrect_few_shot_prompt(data_point, num_shots):\n","    prompt = get_instruction()\n","    for i in range(num_shots):\n","        prompt += make_simple_prompt(train_set[i])\n","        valid_answers = ['A', 'B', 'C', 'D', 'E']\n","        valid_answers.remove(train_set[i]['answerKey'])\n","        # Randomly choose an incorrect answer\n","        random_answer = random.choice(valid_answers)\n","        prompt += f\"{random_answer}\\n\"\n","    prompt += make_simple_qa_prompt(data_point)\n","    return prompt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UrXbp2iMU7re"},"outputs":[],"source":["# Print one example of each prompt type\n","print('='*40, 'Simple Prompt', '='*40)\n","print(make_simple_prompt(train_set[0]))\n","print('='*40, 'Simple QA Prompt', '='*40)\n","print(make_simple_qa_prompt(train_set[0]))\n","print('='*40, 'QA Instruction Prompt', '='*40)\n","print(make_qa_instruction_prompt(train_set[0]))\n","print('='*40, 'Few Shot Prompt', '='*40)\n","print(make_few_shot_prompt(train_set[8], 4))\n","print('='*40, 'Incorrect Few Shot Prompt', '='*40)\n","print(make_incorrect_few_shot_prompt(train_set[8], 4))"]},{"cell_type":"markdown","metadata":{"id":"8NxHU7rQU7re"},"source":["# Running the model\n","\n","The cells below contain code to query Cohere's command-xlarge-nightly model. You can read more about it here: https://docs.cohere.ai/reference/generate.\n","\n","The algorithm is as follows:\n","1. Format the multiple-choice question as a prompt such that the expected continuation is the answer to the question.\n","2. Query the model with the prompt.\n","3. Parse the model's response to extract the answer.\n","\n","\n","Since we are using the trial keys, we are rate limited to 5 queries per minute. We have implemented the code to wait for a minute if you get close to the rate limit. If you run into rate limits, just wait a bit and retry. (Previous queries will stay in the cache, so you won't have to re-query them.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i87UtnWVU7rf"},"outputs":[],"source":["# Parameters for the query.\n","# Run with the defaults first, then if you want try changing them to see how it affects the results\n","model = \"command-xlarge-nightly\"  # This is the biggest (and most expensive) model. You can also try \"xlarge\" or \"medium\"\n","temperature = 0 # Control randomness. For more randomness, set to a higher value. For QA, we recommend 0.0\n","max_tokens = 1 # Only generate 1 token (the answer). The Cohere API\n","num_generations = 5 # How many outputs to generate\n","return_likelihoods = 'GENERATION'  # Return the likelihoods for the generations\n","\n","# To avoid errors due to hitting API rate limits, we'll maintain a running tracker of queries made in the last minute\n","MAX_QUERIES = 5   # 5 queries per 60s in the cohere free tier\n","QUERY_TIMEWINDOW = 60 # seconds\n","timestamps = deque()\n","\n","def query_cohere(prompt):\n","    response = co.generate(\n","      model=model,\n","      prompt=prompt,\n","      temperature=temperature,\n","      max_tokens=max_tokens,\n","      num_generations=num_generations,\n","      return_likelihoods=return_likelihoods,\n","    )\n","    return response\n","\n","\n","# We will store the results in a cache so we don't have to query Cohere every time\n","# This will avoid hitting the API rate limit\n","cache = {}\n","if os.path.exists('cache.pkl'):\n","    with open('cache.pkl', 'rb') as f:\n","        cache = pkl.load(f)\n","\n","def post_process_response_text(response):\n","    \"\"\"Removes trailing and preceding whitespace and newlines. Returns the first character of the response.\"\"\"\n","    r = response.strip()\n","    if r:\n","        return r[0]\n","    return r\n","\n","# Return the log probability of the correct answer and the most probable answer\n","def query_model(prompt, correct_answer):\n","    global query_count, timestamps\n","    # Check if the query is in the cache\n","    inputs = (prompt, correct_answer, model, temperature, max_tokens, return_likelihoods)\n","    if inputs in cache:\n","        response = cache[inputs]\n","    else:\n","        # If more than 5 queries have been run in the last 60s, wait for cooldown\n","        if len(timestamps) >= MAX_QUERIES and time.time() - timestamps[0] <= QUERY_TIMEWINDOW:\n","            print(\"Sleeping for a minute to cooldown API limits.\")\n","            time.sleep(60)\n","            timestamps.clear()\n","\n","        # Run query\n","        response = query_cohere(prompt)\n","\n","        # Update timestamps\n","        timestamps.append(time.time())\n","        if len(timestamps) > MAX_QUERIES:\n","            timestamps.popleft()\n","\n","        # Cache inputs\n","        cache[inputs] = response\n","        # Save cache to file\n","        with open('cache.pkl', 'wb') as f:\n","            pkl.dump(cache, f)\n","    log_prob = response.data[0].likelihood\n","    most_probable = post_process_response_text(response.data[0].text)\n","    return log_prob, most_probable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6q2Q8Sf6U7rf"},"outputs":[],"source":["# For simplicity (and to save time), we'll only use the first 10 data points\n","# You can get more reliable results by using more data points, but it will\n","# take longer to run because of API rate limits.\n","num_points = 10\n","mini_train = val_set[:num_points]\n","num_shots = 4\n","\n","# Consider adding additional prompts of your own here.\n","prompt_strategies = {\n","    'simple': make_simple_prompt,\n","    'simple_qa': make_simple_qa_prompt,\n","    'qa_instruction': make_qa_instruction_prompt,\n","    'few_shot': lambda x: make_few_shot_prompt(x, num_shots),\n","    'incorrect_few_shot': lambda x: make_incorrect_few_shot_prompt(x, num_shots)\n","}\n","\n","def compute_acc(data_points, prompt_strategy):\n","    accuracies = []\n","    valid_responses = []\n","    for data_point in data_points:\n","        print(f'Question: {data_point[\"question\"][\"stem\"]}, Answer: {data_point[\"answerKey\"]}, Choices: {[choice[\"text\"] for choice in data_point[\"question\"][\"choices\"]]}')\n","        prompt = prompt_strategy(data_point)\n","        correct_answer = f'{data_point[\"answerKey\"]}'\n","\n","        log_prob, most_probable = query_model(prompt, correct_answer)\n","        accuracies += [int(most_probable == correct_answer)]\n","        valid_responses += [most_probable in ['A', 'B', 'C', 'D', 'E']]\n","        print(f'   LM predicted |{most_probable}|, accuracy: {accuracies[-1]}')\n","    return np.mean(accuracies), np.mean(valid_responses)\n","\n","def plot_all_accs(data_points, prompt_strategies):\n","    accuracies = []\n","    valid_responses = []\n","    for prompt_name, prompt_strategy in prompt_strategies.items():\n","        accuracy, valid_rate = compute_acc(data_points, prompt_strategy)\n","        accuracies += [accuracy]\n","        valid_responses += [valid_rate]\n","    # Plot a bar chart of the accuracies\n","    plt.figure(figsize=(10, 5))\n","    plt.bar(prompt_strategies.keys(), accuracies)\n","    plt.title('Accuracies')\n","    plt.show()\n","    # Plot a bar chart of the valid responses\n","    plt.figure(figsize=(10, 5))\n","    plt.bar(prompt_strategies.keys(), valid_responses)\n","    plt.title('Valid Responses')\n","    plt.show()\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NlZ9rNdCU7rg"},"outputs":[],"source":["plot_all_accs(mini_train, prompt_strategies)"]},{"cell_type":"markdown","metadata":{"id":"Ic7JH8WTU7rg"},"source":["# Analysis\n","\n","TODO: in the cells below, implement code to analyze the model's performance.\n","* What kinds of failures do you see with different prompting strategies?\n","* Does providing correct labels in few-shot prompting have a significant impact on accuracy?\n","* Observe the model’s log probabilities. Does it seem more confident when it is correct than when it is incorrect?\n","\n","A function to plot the model's confidence has been implemented for you, but you should feel free to write code to do additional analysis.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BWnuRosfU7rg"},"outputs":[],"source":["# Printing the log probs for the generated tokens\n","prompt_strategies = {\n","    # 'simple': make_simple_prompt, # removed since it's always incorrect\n","    'simple_qa': make_simple_qa_prompt,\n","    'qa_instruction': make_qa_instruction_prompt,\n","    'few_shot': lambda x: make_few_shot_prompt(x, num_shots),\n","    'incorrect_few_shot': lambda x: make_incorrect_few_shot_prompt(x, num_shots)\n","}\n","\n","argmax_lp_correct = []\n","argmax_lp_incorrect = []\n","for prompt_name, prompt_strategy in prompt_strategies.items():\n","    argmax_lp_correct_prompt = []\n","    argmax_lp_incorrect_prompt = []\n","    for data_point in val_set[:num_points]:\n","        prompt = prompt_strategy(data_point)\n","        correct_answer = f'{data_point[\"answerKey\"]}'\n","        print(f\"Prompt: {prompt}\")\n","        log_probs, most_probable = query_model(prompt, correct_answer)\n","        log_prob_argmax = log_probs\n","        if most_probable == correct_answer:\n","            argmax_lp_correct_prompt += [log_prob_argmax]\n","        else:\n","            argmax_lp_incorrect_prompt += [log_prob_argmax]\n","        print(f'LM predicted |{most_probable}|, correct answer: {correct_answer}')\n","\n","    argmax_lp_correct += [np.mean(argmax_lp_correct_prompt)]\n","    argmax_lp_incorrect += [np.mean(argmax_lp_incorrect_prompt)]\n","\n","# Plot a bar chart of the accuracies. The bar chart has two bars for each prompt strategy.\n","# These bars are placed side by side, so you can compare the accuracies of the two strategies.\n","plt.figure(figsize=(10, 5))\n","plt.bar(np.arange(len(prompt_strategies)) - 0.2, np.exp(argmax_lp_correct), width=0.4, label='Correct')\n","plt.bar(np.arange(len(prompt_strategies)) + 0.2, np.exp(argmax_lp_incorrect), width=0.4, label='Incorrect')\n","plt.title('Confidence (Probability of most likely token)')\n","plt.xticks(np.arange(len(prompt_strategies)), prompt_strategies.keys())\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"z16OJFJfU7rh"},"source":["# Training GPT-2 for soft prompt tuning\n","\n","GPT-2 is the smaller predecessor model to GPT-3. We will use GPT-2 for soft prompt tuning as it is\n","publicly available(unlike GPT-3) and small enough to train on the free version of the colab GPU\n","(unlike GPT-J).\n","\n","Soft prompt tuning is described in this [paper](https://arxiv.org/abs/2104.08691v1), which we encourage you to learn more about. In essence, instead of generating answers by putting in token prompts, we use fine tuning to train the embeddings of new learned tokens. This allows us to generate answers by putting in the new learned tokens instead of tokens which correspond to real words.\n","\n","Most of the code has been implemented for you, but you should still read through the code to understand what it's doing. There is one TODO which asks you to set up the optimizer. Think about which parameters should get passed into the optimizer.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wKSHHwAXU7rh"},"outputs":[],"source":["#@title Define soft embedding for GPT-2\n","#@markdown code adapted from [this github repo](https://github.com/kipgparker/soft-prompt-tuning) implementing the [soft prompt tuning paper](https://arxiv.org/abs/2104.08691v1)\n","class SoftEmbedding(nn.Module):\n","    def __init__(self,\n","                wte: nn.Embedding,\n","                n_tokens: int = 10,\n","                random_range: float = 0.5,\n","                initialize_from_vocab: bool = True):\n","        \"\"\"\n","        Here, we concatentate a new task-specific learned embedding to the existing GPT-2 embedding.\n","        Args:\n","            wte (nn.Embedding): original transformer word embedding\n","            n_tokens (int, optional): number of tokens for task. Defaults to 10.\n","            random_range (float, optional): range to init embedding (if not initialize from vocab). Defaults to 0.5.\n","            initialize_from_vocab (bool, optional): initalizes from default vocab. Defaults to True.\n","        \"\"\"\n","        super(SoftEmbedding, self).__init__()\n","        self.wte = wte\n","        self.n_tokens = n_tokens\n","        self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(wte,\n","                                                                               n_tokens,\n","                                                                               random_range,\n","                                                                               initialize_from_vocab))\n","\n","    def initialize_embedding(self,\n","                             wte: nn.Embedding,\n","                             n_tokens: int = 10,\n","                             random_range: float = 0.5,\n","                             initialize_from_vocab: bool = True):\n","        \"\"\"initializes learned embedding\n","        Args:\n","            same as __init__\n","        Returns:\n","            torch.float: initialized using original schemes\n","        \"\"\"\n","        if initialize_from_vocab:\n","            return self.wte.weight[:n_tokens].clone().detach()\n","        return torch.FloatTensor(n_tokens, wte.weight.size(1)).uniform_(-random_range, random_range)\n","\n","    def forward(self, tokens):\n","        \"\"\"run forward pass\n","        Args:\n","            tokens (torch.long): input tokens before encoding\n","        Returns:\n","            torch.float: encoding of text concatenated with learned task specifc embedding\n","        \"\"\"\n","        # The first n_tokens embeddings are reserved for the learned embeddings\n","        # The rest of the embeddings are the original GPT-2 embeddings\n","        input_embedding = self.wte(tokens[:, self.n_tokens:])\n","        learned_embedding = self.learned_embedding.repeat(input_embedding.size(0), 1, 1)\n","        return torch.cat([learned_embedding, input_embedding], 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3TWAX7kEU7ri"},"outputs":[],"source":["#@markdown Set up a soft embedding version of GPT-2\n","from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n","tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\", padding_side='left')\n","tokenizer.pad_token=tokenizer.eos_token\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_tokens = 100\n","\n","def initialize_soft_model():\n","    model = GPT2LMHeadModel.from_pretrained('gpt2')\n","\n","    initialize_from_vocab = True\n","\n","    # Set the input embeddings to the GPT2 model\n","    s_wte = SoftEmbedding(model.get_input_embeddings(),\n","                        n_tokens=n_tokens,\n","                        initialize_from_vocab=initialize_from_vocab)\n","    model.set_input_embeddings(s_wte)\n","    model.to(device)\n","    return model\n","\n","# While we didn't need to do this for GPT-3 earlier, training the model means we need to turn the text\n","# into tokens that the model can understand via the embedding layer.\n","def process_dataset(dataset, mapper_fn, pad_length=119):\n","    mapped_dataset = [mapper_fn(item) for item in dataset]\n","    if pad_length is None:\n","        out = tokenizer(mapped_dataset, return_tensors='pt', padding=True)\n","    else:\n","        out = tokenizer(mapped_dataset, return_tensors='pt', padding=\"max_length\", max_length=pad_length)\n","    # Need to add a space as GPT differentiates between \" A\" and \"A\" and it will be predicting \" A\".\n","    answerkey = [' ' + item['answerKey'] for item in dataset]\n","    out['answerkey'] = tokenizer(answerkey, return_tensors='pt', max_length=1)['input_ids']\n","    return out\n","\n","def pad_soft_inputs(inputs):\n","    \"\"\"\n","    We need to pad the attention_mask and input_ids with an extra n_learned_tokens\n","    It does not matter what you pad input_ids with since these will be overwritten by learned embeddings\n","    \"\"\"\n","    batch = len(inputs['input_ids'])\n","    inputs['input_ids'] = torch.cat([torch.full((batch, n_tokens), 50256).to(device), inputs['input_ids'].to(device)], 1).to(device)\n","    inputs['attention_mask'] = torch.cat([torch.full((batch, n_tokens), 1).to(device), inputs['attention_mask'].to(device)], 1).to(device)\n","    return inputs\n","\n","# Train the model\n","def train_model(model, train_set, val_set, dataset_processor, batch_size=8, epochs=1, lr=1e-4, print_every=100):\n","    train_dataset = process_dataset(train_set, dataset_processor)\n","    val_dataset = process_dataset(val_set, dataset_processor)\n","    parameters_to_train = # TODO\n","    optimizer = torch.optim.Adam(parameters_to_train, lr=1e-4)\n","    criterion = nn.CrossEntropyLoss()\n","    model.to(device)\n","    model.train()\n","    epoch_train_losses = []\n","    for i in range(epochs):\n","        epoch_loss = 0\n","        for j in range(0, len(train_dataset['input_ids']), batch_size):\n","            # Calculate cross entropy loss between predicted last token and actual last token\n","            optimizer.zero_grad()\n","            inputs = {k: v[j:j+batch_size].to(device) for k, v in train_dataset.items()}\n","            inputs = pad_soft_inputs(inputs)\n","            labels = inputs.pop('answerkey')\n","            outputs = model(**inputs).logits[:, -1, :] # (batch_size, vocab_size)\n","            loss = criterion(outputs, labels.squeeze())\n","            loss.backward()\n","            optimizer.step()\n","            # loss calculated by criterion is averaged over batch, so multiply by batch size to get total loss\n","            epoch_loss += loss.item() * labels.shape[0]\n","            if j % print_every == 0:\n","                print(f'Epoch {i}, Item {j}, loss: {loss.item()}')\n","        epoch_loss /= train_dataset['input_ids'].shape[0]\n","        epoch_train_losses.append(epoch_loss)\n","\n","        # Evaluate on validation set\n","        model.eval()\n","        val_loss = 0\n","        for j in range(0, len(val_dataset['input_ids']), batch_size):\n","            inputs = {k: v[j:j+batch_size].to(device) for k, v in val_dataset.items()}\n","            inputs = pad_soft_inputs(inputs)\n","            labels = inputs.pop('answerkey')\n","            outputs = model(**inputs).logits[:, -1, :]\n","            if j == 0: print(f'decoding {tokenizer.decode(outputs.argmax(dim=-1))}')\n","            loss = criterion(outputs, labels.squeeze())\n","            val_loss += loss.item() * labels.shape[0]\n","        val_loss /= val_dataset['input_ids'].shape[0]\n","        print('-'*20)\n","        print(f'Epoch {i}, Validation loss: {val_loss}')\n","\n","\n","# This function lets us sample the next token (or, in our case, the next answer) from the model.\n","def generate_output(model, inputs, pad_soft=True):\n","    \"\"\"\n","    Given a string text or a tokenized input (or list of these, if batched), returns the model's prediction for the\n","    next token in the sequence.\n","    \"\"\"\n","    model.eval()\n","    if type(inputs) is str or type(inputs) is list and type(inputs[0]) is str:\n","        inputs = tokenizer(inputs, return_tensors=\"pt\").to(device)\n","\n","    if pad_soft:\n","        inputs = pad_soft_inputs(inputs)\n","    outputs = model(**inputs).logits[0, -1, :]\n","    outputs = outputs.argmax(dim=-1)\n","    return tokenizer.decode(outputs)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E4YnMpBqI45R"},"outputs":[],"source":["model = initialize_soft_model()\n","hard_embedding_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n","print(generate_output(hard_embedding_model, 'Deep learning is an', pad_soft=False))\n","\n","# Print out the embeddings so you can see their shpaes\n","print('Embedding object', model.get_input_embeddings())\n","print('Learned embeddings', model.get_input_embeddings().learned_embedding.shape)\n","print('Original vocab embeddings', model.get_input_embeddings().wte.weight.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UCZQZU17XuJi"},"outputs":[],"source":["prompt_strategy = 'qa_instruction'\n","if prompt_strategy not in prompt_strategies:\n","    print('prompt strategy must be one of', [i for i in prompt_strategies])\n","else:\n","    prompt_strategy = prompt_strategies[prompt_strategy]\n","    train_model(model, train_set, val_set, prompt_strategy, batch_size=8, epochs=2) # you can reduce the batch size if you run out of memory\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xWzIveVzI45R"},"outputs":[],"source":["# Save the model to a pickle file (If your runtime crashes, you can load the model from this file)\n","with open('soft_embeddings_model_qa.pkl', 'wb') as f:\n","    pkl.dump(model, f)"]},{"cell_type":"markdown","metadata":{"id":"TorOKrwaI45R"},"source":["Compare the performance of the model with hard prompting and with soft prompt tuning. If your implementation is correct, you should get around 21% correct and 0% invalid with the soft prompt. Answer the analysis questions in the written portion of the assignment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3J3PS-70U7rj"},"outputs":[],"source":["#@title Let's compare a few examples from our prompts\n","def compare_models(model1, model1_pad_soft, model2, model2_pad_soft, dataset, dataset_processor, n_entries=5):\n","    processed_set = process_dataset(dataset, dataset_processor)\n","    for i in range(n_entries):\n","        data_point_idx = np.random.choice(len(dataset))\n","        point = dataset[data_point_idx]\n","        processed_point = processed_set[data_point_idx]\n","        prompt = dataset_processor(point)\n","        print(prompt)\n","        output1 = generate_output(model1, dataset_processor(point), pad_soft=model1_pad_soft)\n","        output2 = generate_output(model2, dataset_processor(point), pad_soft=model2_pad_soft)\n","        print(f'gt: {point[\"answerKey\"]}. model1: {repr(output1)}. model2: {repr(output2)}')\n","        print('-' * 20)\n","\n","\n","compare_models(model, True, hard_embedding_model, False, val_set, prompt_strategy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cfXUEeJmU7rj"},"outputs":[],"source":["#@title Bar plot the distribution of incorrect answers, invalid answers, and correct answers\n","def get_answer_stats(model, dataset, dataset_processor, verbose, pad_soft):\n","    processed_set = process_dataset(dataset, dataset_processor)\n","    correct = 0\n","    incorrect = 0\n","    invalid = 0\n","    for i in range(len(dataset)):\n","        inputs = {k: v[i:i+1].to(device) for k, v in processed_set.items()}\n","        point = dataset[i]\n","        prompt = dataset_processor(point)\n","        label = inputs.pop('answerkey')\n","        output = generate_output(model, inputs, pad_soft).strip()\n","        if verbose: print(f'Prompt: {prompt}, output: |{output}|, answerkey |{point[\"answerKey\"]}|')\n","        if output == point[\"answerKey\"]:\n","            correct += 1\n","        elif output in ['A', 'B', 'C', 'D', 'E'] or not point['answerKey'] in ['A', 'B', 'C', 'D', 'E']:\n","            incorrect += 1\n","        else:\n","            invalid += 1\n","    correct, incorrect, invalid = correct/len(dataset), incorrect/len(dataset), invalid/len(dataset)\n","    return correct, incorrect, invalid\n","\n","def plot_answer_stats(model, dataset, dataset_processor, verbose=False, pad_soft=True):\n","    correct, incorrect, invalid = get_answer_stats(model, dataset, dataset_processor, verbose, pad_soft)\n","    plt.bar(['correct', 'incorrect', 'invalid'], [correct, incorrect, invalid])\n","    plt.title('Answer distribution')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4F-l72jcU7rj"},"outputs":[],"source":["plot_answer_stats(model, val_set, prompt_strategy, verbose=True, pad_soft=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ZY_SuePeBK4"},"outputs":[],"source":["plot_answer_stats(hard_embedding_model, val_set, prompt_strategies['qa_instruction'], pad_soft=False, verbose=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JGiiewORI45R"},"outputs":[],"source":["model_soft_qa = model"]},{"cell_type":"markdown","metadata":{"id":"vZGRQUvdI45R"},"source":["# Pluralize task\n","\n","As you can see above, the soft embedding model does not perform very well on this task. We'll show how soft prompting does better on a second, very simple task - pluralizing a word.\n","\n","The dataset we use was found here, and consists of a list of English nouns: https://www.kaggle.com/datasets/leite0407/list-of-nouns?select=nounlist.csv.\n","For simplicity, we will only consider words where the output is a single token (to avoid needing to deal with sequential generation for evaluation), but you could adapt the code to generate arbitrarily long outputs.\n","\n","If you get memory errors when running this part, re-run the notebook while skipping loading the previous dataset and soft model.\n","\n","\n","!! If you run into an error during training complaining about batch size dimensions, this is an edge-case issue where we get errors when the last batch in an epoch length 1. You can fix this by removing the item in the train set. !!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dBi3QVllI45R"},"outputs":[],"source":["!curl https://inst.eecs.berkeley.edu/~cs182/fa22/assets/assignments/nounlist.csv -o nounlist.csv"]},{"cell_type":"markdown","metadata":{"id":"FzAeBRBFI45R"},"source":["We'll create targets for this dataset using the inflect library, which is a Python library for inflecting English words. You can read more about it here: https://pypi.org/project/inflect/. This library can convert word to plural forms (though it is not 100% reliable)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X2gUrggh_XAl"},"outputs":[],"source":["# Set of words with unusual plurals\n","noun_test = ['foot', 'man', 'person', 'self', 'wife', 'wolf', 'woman']\n","\n","engine = inflect.engine()\n","\n","# Load new noun_list dataset from csv file\n","with open('nounlist.csv', 'r') as f:\n","    noun_list = f.read().splitlines()\n","    noun_list = [i.strip() for i in noun_list]\n","random.seed(0)\n","# shuffle the noun list\n","random.shuffle(noun_list)\n","# Remove all list items which are in the nouns list (our test set)\n","noun_list = [i for i in noun_list if i not in noun_test]\n","# Remove the last 10% for validation\n","noun_train = noun_list[:-int(len(noun_list)*0.1)]\n","noun_val = noun_list[-int(len(noun_list)*0.1):]\n","\n","# Plural task\n","def format_dataset(noun_list):\n","    dataset = []\n","    for noun in noun_list:\n","        plural = engine.plural(noun)\n","        dataset.append({'answerKey': plural, 'input': noun})\n","    return dataset\n","\n","\n","noun_train = format_dataset(noun_train)\n","noun_val = format_dataset(noun_val)\n","noun_test = format_dataset(noun_test)\n","\n","\n","# Only include nouns where the plural is a single token\n","noun_train = [i for i in noun_train if len(tokenizer(i['answerKey'])['input_ids']) == 1]\n","noun_val = [i for i in noun_val if len(tokenizer(i['answerKey'])['input_ids']) == 1]\n","noun_test = [i for i in noun_test if len(tokenizer(i['answerKey'])['input_ids']) == 1]\n","\n","# Print the first 10 items in the dataset\n","print([point['input'] for point in noun_train[:10]])\n","print([point['answerKey'] for point in noun_train[:10]])\n","\n","print(f'Lengths: train: {len(noun_train)}, val: {len(noun_val)}, test: {len(noun_test)}')"]},{"cell_type":"markdown","metadata":{"id":"cCpYm0JEI45S"},"source":["Compare hard prompting with soft prompting on this task, then answer the analysis questions in the written part of this homework. You should get over 60% correct on the val set with soft prompting."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZE5sX6DSI45S"},"outputs":[],"source":["def basic_format(point):\n","    if isinstance(point, dict):\n","        point = point['input']\n","    return f\"The plural of {point} is\"\n","\n","examples = ['pasta', 'sweater', 'wave', 'mouse', 'attorney', 'bottle', 'phone', 'grass', 'evening', 'candy', 'flower', 'planet', 'architect', 'washer',\n","            'keyhole', 'economy', 'grace', 'finance', 'midnight', 'cushion', 'plateau', 'mouse', 'chord', 'cactus', 'swap', 'tremor', 'criterion', 'sink', 'bandana', 'trade'\n","            ]\n","\n","def make_few_shot(i):\n","    def few_shot(point):\n","        prompt = ''\n","        for j in range(i):\n","            prompt += basic_format(examples[j]) + ' ' + engine.plural(examples[j]) + '.\\n'\n","        prompt += basic_format(point)\n","        return prompt\n","    return few_shot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-f3LXo1pI45S"},"outputs":[],"source":["model_pluralize = initialize_soft_model()\n","train_model(model_pluralize, noun_train, noun_val, basic_format, batch_size=8, epochs=40)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9-Fm5R2OI45S"},"outputs":[],"source":["# Save the model to a pickle file (If your runtime crashes, you can load the model from this file)\n","with open('soft_embeddings_model_pluralize.pkl', 'wb') as f:\n","    pkl.dump(model, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3A80lhQjI45S"},"outputs":[],"source":["plot_answer_stats(model_pluralize, noun_val, basic_format, verbose=True, pad_soft=True)\n","plot_answer_stats(model_pluralize, noun_test, basic_format, verbose=True, pad_soft=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"byiaYr9WI45S"},"outputs":[],"source":["# Plot results with hard prompts of various lengths\n","for num_shots in range(10):\n","    plot_answer_stats(hard_embedding_model, noun_val, make_few_shot(num_shots), verbose=False, pad_soft=True)"]},{"cell_type":"markdown","metadata":{"id":"cnEosJ5BI45S"},"source":["# Deliverables\n","\n","Please submit this completed notebook and complete all the written questions."]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1jNv7fs0wfGNZxif1MOVnL-_dPhMAeoLS","timestamp":1692845739993}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.7.13 ('cs182_hw88')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"24c732284262cd41d749de7c2ab26fba11a555a98d922bf877eca389ddd668a1"}}},"nbformat":4,"nbformat_minor":0}