{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1gvx9kCUSJsHZ_FiMlN8PuHyLfC3awO-g","timestamp":1692845769923}]}},"cells":[{"cell_type":"markdown","source":["#**Q. Zero order optimization (Policy Gradient)**\n","\n","We will now talk about $0^{th}$ order optimization, also known as Policy Gradient in a Reinforcement Learning context. Although this method is primarily used in an RL context we will be adapting this method to do $0^{th}$ order optimization on a Neural Network.\n","\n","$k^{th}$ order optimization means that in the optimization, we use a $k^{th}$ order derivative ($\\frac{δL^k}{δ^kw}$) to do the optimization. So we can see that gradient descent is a first order optimization method, while Newton's method is a second order optimization method.\n","\n","Polciy gradient is a $0^{th}$ order optimization method - which means that you use no derivative for the optimzation. This is used in contexts in which the loss is a **blackboxed** function, hence propogating a gradient through it is impossible.\n","\n","Policy gradient at a high level approximates the gradient and then does gradient descent using this approximated gradient."],"metadata":{"id":"ZBeJ1fQyahdK"}},{"cell_type":"markdown","source":["##**a) A handy derivation**\n","Prove that $p_{\\theta}(x) \\nabla_θlog(p_{\\theta}(x)) = \\nabla_θp_{\\theta}(x)$\n"],"metadata":{"id":"LtPpvzjUahdM"}},{"cell_type":"markdown","source":["##**b) Approximating the derivative**\n","Let's say we have a neural network $f(x)$ which takes in a $x$ and uses the weights($w$) to output 2 logits <br> ($P = [P(y = 0)$, $P(y = 1)]$). <br> Let $p(x, y)$ be the joint distribution of the input and output data according to **our model**. Hence $p_w(x, y) = p(x)p_w(y|x)$, where p(x) is the ground distribution of x, while $p_w(y|x) = f(x)[y]$ is what our model predicts.\n","<br><br>\n","\n","Similarly we have a **blackboxed** loss function $L(x, f(x))$ which outputs a loss. <br>\n","For example if i wanted to learn to classify y = 1 if x > 5 and y = 0 otherwise, L(4, (0.1, 0.9)) would be small while L(4, (0.9, 0.1)) would be very high. As we already discussed, since this loss is blackboxed we can't take the derivative through it.\n","<br><br>\n","We want to optimize the following objective function <br>\n","$w^* = argmin_wJ(w)$ <br> where $J(w) = E_{(x, f(x)) \\sim p_w(x, y)}[L(x, f(x))]$.\n","<br>\n","To do this optimization we want to approximate $\\nabla_{w} J(w)$ so that we could use an optimization method like gradient descent to find $w^*$\n","<br><br>\n","**Prove that $\\nabla_{w} J(w)$ can be approximated as $\\frac{1}{N}∑_{i=1}^{i=N}(\\nabla_wlog(p_w(y_i|x_i))L(x_i, f(x_i))$**\n","<br><br>\n","**HINTS:**\n","<ol>\n","<li>Try creating a $\\tau = (x, f(x))$</li>\n","<li>$E[X] = \\int_xxP(X=x)dx$ </li>\n","<li>Use the result from part a which was $p_{\\theta}(x) \\nabla_θlog(p_{\\theta}(x)) = \\nabla_θp_{\\theta}(x)$</li>\n","<li>$p_w(x, y) = p(x)p_w(y|x)$</li>\n","</ol>"],"metadata":{"id":"YyhoFOTSahdM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RVkC60-2ahdN"},"outputs":[],"source":["from collections import OrderedDict\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch as torch\n","import torch.nn.functional as F\n","from torch import nn"]},{"cell_type":"code","source":["# Determine the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device\", device)"],"metadata":{"id":"kljXk-yoahdO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data Generation\n","\n","In this question, each datapoint is a 8 dimensional vector assigned to one of the four labels depending on their distance to two points $\\mathbf{1}$ and $-\\mathbf{1}$."],"metadata":{"id":"hNPIU6vVahdO"}},{"cell_type":"code","source":["def generate_data(num_samples, input_dim):\n","    @torch.no_grad()\n","    def true_y(x):\n","        dp = ((x - 1) ** 2).sum(dim=-1)\n","        dn = ((x + 1) ** 2).sum(dim=-1)\n","        zp = dp <= input_dim * 2.5\n","        zn = dn <= input_dim * 2.5\n","\n","        return torch.stack([\n","            zp & zn,\n","            zp & ~zn,\n","            ~zp & zn,\n","            ~zp & ~zn\n","        ], dim=1).long().argmax(dim=-1)\n","\n","    x = torch.rand((num_samples, input_dim)) * 4.4 - 2.2\n","    y = true_y(x)\n","    return x, y, true_y"],"metadata":{"id":"lwGg9fg_ahdO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here is a visualization when the input dimension is 2:"],"metadata":{"id":"Gl8NwUF5ahdO"}},{"cell_type":"code","source":["x, y, true_y = generate_data(1000, 2)\n","plt.scatter(x[:, 0].numpy(), x[:, 1].numpy(), c=y.numpy(), s=5)"],"metadata":{"id":"RZg3QuQuahdO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's generate data for input dimension of 8."],"metadata":{"id":"vh2vExpXahdO"}},{"cell_type":"code","source":["torch.manual_seed(73)\n","np.random.seed(73)\n","x, y, true_y = generate_data(1000, 8)\n","x_test, y_test, _ = generate_data(1000, 8)\n","x, y = x.to(device), y.to(device)\n","x_test, y_test = x_test.to(device), y_test.to(device)"],"metadata":{"id":"np2220ueahdO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here is the definition of our model:"],"metadata":{"id":"iJ4nsJsSahdO"}},{"cell_type":"code","source":["def get_model(seed):\n","    torch.manual_seed(seed)\n","    np.random.seed(seed)\n","    return nn.Sequential(\n","        OrderedDict(\n","            [  # randomly initialized NN\n","                ('fc1', nn.Linear(8, 32)),\n","                ('relu1', nn.ReLU()),\n","                ('output', nn.Linear(32, 4))]\n","        )\n","    ).to(device)"],"metadata":{"id":"kMCfuLxeahdP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here is the definition of our metric: accuracy."],"metadata":{"id":"HyWzJ9EKahdP"}},{"cell_type":"code","source":["@torch.no_grad()\n","def accuracy(model, x, y):\n","    pred = torch.argmax(model(x), dim=1)\n","    correct = torch.sum(pred == y).item()\n","    acc = (correct / len(y)) * 100\n","    return acc"],"metadata":{"id":"kRpt_cfUahdP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Supervised Learning"],"metadata":{"id":"3EPhZcWiahdP"}},{"cell_type":"code","source":["lr = 0.002\n","num_iters = 3000\n","model = get_model(73)\n","optimizer = torch.optim.Adam(model.parameters(), lr)\n","train_accs = []\n","train_accs.append(accuracy(model, x, y))\n","valid_accs = []\n","valid_accs.append(accuracy(model, x_test, y_test))\n","for epoch in range(num_iters):\n","    logits = model(x)\n","    criterion = nn.CrossEntropyLoss()\n","    loss = criterion(logits, y)\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    train_accs.append(accuracy(model, x, y))\n","    valid_accs.append(accuracy(model, x_test, y_test))"],"metadata":{"id":"kqqeLgHzahdP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot([i for i in range(len(train_accs))], train_accs, label=\"Train\")\n","plt.plot([i for i in range(len(valid_accs))], valid_accs, label=\"Test\")\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.show()\n","print(\"Final test accuracy:\", valid_accs[-1])"],"metadata":{"id":"xL6CrWIFahdP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Policy Gradient"],"metadata":{"id":"4181Vf4ZahdP"}},{"cell_type":"code","source":["# Reward Oracle Function (Black-boxed)\n","# This function calculates the reward, returning 3 for correct predictions and -1 for incorrect ones.\n","# Usage guidelines:\n","# - Call this function only; do not rely on its internal implementation details.\n","# - Gradients are not calculated within this function due to the `@torch.no_grad()` decorator.\n","@torch.no_grad()\n","def reward_oracle(x, pred):\n","    return torch.where(true_y(x) == pred, 3, -1)"],"metadata":{"id":"4Fxj50JRahdP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Implement Policy Gradient Algorithm:** Based on the formulas derived in part (b), complete the policy gradient implementation. For this task, we will use the Adam optimizer and process the full dataset in a single batch. The reward oracle has been invoked for you in the provided code. Remember, do not use the true labels directly in your implementation.\n","\n","*Hint: there are two approaches to get `logprob_on_predicted`: (a) use `torch.gather`. (b) use `nn.CrossEntropyLoss` (or its equivalent in `torch.nn.functional`), but with predicted labels instead of true labels.*"],"metadata":{"id":"rCxct1tAahdP"}},{"cell_type":"code","source":["lr = 0.002\n","num_iters = 3000\n","model = get_model(73)\n","optimizer = torch.optim.Adam(model.parameters(), lr)\n","train_accs = []\n","train_accs.append(accuracy(model, x, y))\n","valid_accs = []\n","valid_accs.append(accuracy(model, x_test, y_test))\n","for epoch in range(num_iters):\n","    logits = model(x)\n","    logprob = F.log_softmax(logits, dim=-1)\n","\n","    predicted = logits.detach().argmax(dim=-1)\n","    reward = reward_oracle(x, predicted)\n","\n","    ############################################################################\n","    # TODO: Calculate the loss function of policy gradient\n","    ############################################################################\n","    logprob_on_predicted = None\n","\n","    loss = None\n","    ############################################################################\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    train_accs.append(accuracy(model, x, y))\n","    valid_accs.append(accuracy(model, x_test, y_test))"],"metadata":{"id":"Yp7o8V4cahdP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Question\n","\n","**Include the screenshot of the accuracy plot** in your written assignment submission. With a correct implementation, you should observe a test accuracy of approximately 75% after the final iteration."],"metadata":{"id":"dI6ah63CahdP"}},{"cell_type":"code","source":["plt.plot([i for i in range(len(train_accs))], train_accs, label=\"Train\")\n","plt.plot([i for i in range(len(valid_accs))], valid_accs, label=\"Test\")\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.show()\n","print(\"Final test accuracy:\", valid_accs[-1])"],"metadata":{"id":"_r7vs-pLahdP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Question\n","\n","**Compare the policy gradient and supervised learning approaches for this classification task, focusing on their convergence speed, stability, and final performance. Explain any observed differences.** Include your response to this question in your written assignment submission."],"metadata":{"id":"r5eGnPnuahdP"}}]}