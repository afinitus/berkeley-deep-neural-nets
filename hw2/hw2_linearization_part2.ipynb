{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dDjGgNibKGlI"},"outputs":[],"source":["import functools\n","import json\n","import math\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim"]},{"cell_type":"markdown","metadata":{"id":"OTNWSlWDKGlL"},"source":["# Generate Training and Test Data\n","\n","The true function that we would like to fit using our neural network is a superposition of 5 sine waves of different frequencies. The resulting function $f_\\text{true}$ is a smooth odd function. Our training data has added Gaussian noise $y = f_\\text{true}(x) + \\epsilon,\\, \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$. The test data is noise-free."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGqQuJZoKGlM"},"outputs":[],"source":["# Define the true function\n","\n","np.random.seed(0)\n","F_TRUE_PARAMS = np.random.rand(5) * 2.0 - 1.0\n","\n","\n","def f_true(x):\n","    m = F_TRUE_PARAMS.shape[0]\n","    return (np.sin(x[:, None] * np.arange(m)[None, :] * math.pi) * F_TRUE_PARAMS).sum(axis=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cBHTvO6FKGlM"},"outputs":[],"source":["plt.figure()\n","plt.plot(np.linspace(-1, 1, 200), f_true(np.linspace(-1, 1, 200)))\n","plt.title(\"groundtruth function\")\n","plt.xlabel(\"x\")\n","plt.ylabel(\"y\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_-lYqx_NKGlM"},"outputs":[],"source":["# Generate noisy training data\n","np.random.seed(10)\n","TRAIN_N = 1500\n","TRAIN_SIGMA = 0.1\n","train_x = np.random.uniform(low=-1.0, high=1.0, size=TRAIN_N)\n","train_y = f_true(train_x) + np.random.randn(TRAIN_N) * TRAIN_SIGMA\n","\n","plt.figure()\n","plt.scatter(train_x, train_y, s=3)\n","plt.title(\"training data\")\n","plt.xlabel(\"x\")\n","plt.ylabel(\"y\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jxIx57JUKGlN"},"outputs":[],"source":["# Generate noiseless test data\n","np.random.seed(10)\n","TEST_N = 500\n","test_x = np.linspace(-1, 1, TEST_N)\n","test_y = f_true(test_x)"]},{"cell_type":"markdown","metadata":{"id":"LJS0jU7qKGlN"},"source":["## Simple Neural Network and Visualization\n","\n","This section is a simplified version of what you have done in the homework 1.\n","We train a 1-hidden layer neural network using Stochastic Gradient Descent with Momentum."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8S-DltyKKGlO"},"outputs":[],"source":["def train_model(model, n_steps, batch_size, lr, seed, train_x=train_x, train_y=train_y):\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    log_interval = 10\n","    eval_interval = int(TRAIN_N / batch_size) + 1\n","    optimizer = optim.SGD(model.parameters(), lr, momentum=0.9)\n","    criterion = nn.MSELoss()\n","    all_indices = []\n","    while len(all_indices) < n_steps * batch_size:\n","        all_indices.append(np.random.permutation(TRAIN_N))\n","    all_indices = np.concatenate(all_indices)\n","    for step in range(n_steps):\n","        indices = all_indices[step * batch_size: step * batch_size + batch_size]\n","        batch_x = torch.from_numpy(train_x[indices]).float().unsqueeze(-1)\n","        batch_y = torch.from_numpy(train_y[indices]).float()\n","        pred = model(batch_x).view(-1)\n","        loss = criterion(batch_y, pred)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        if (step + 1) % log_interval == 0:\n","            print(json.dumps({\"step\": step, \"train_loss\": loss.item()}))\n","        if (step + 1) % eval_interval == 0 or step + 1 == n_steps:\n","            with torch.no_grad():\n","                batch_x = torch.from_numpy(test_x).float().unsqueeze(-1)\n","                batch_y = torch.from_numpy(test_y).float()\n","                pred = model(batch_x).view(-1)\n","                loss = criterion(batch_y, pred)\n","                print(json.dumps({\"step\": step, \"test_loss\": loss.item()}))\n","    return model, batch_x.view(-1).numpy(), pred.numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g1Q_mL8YKGlP"},"outputs":[],"source":["# Train a neural network with 1 hidden layer\n","\n","width = 20\n","lr = 0.1\n","batch_size = 32\n","n_steps = int(TRAIN_N / batch_size * 10)\n","\n","np.random.seed(100)\n","torch.manual_seed(100)\n","\n","model = nn.Sequential(\n","    nn.Linear(1, width),\n","    nn.ReLU(),\n","    nn.Linear(width, 1)\n",")\n","\n","model, evaluated_x, evaluated_pred = train_model(model, n_steps, batch_size, lr, 150)\n","\n","plt.figure()\n","plt.plot(test_x, f_true(test_x), label=\"truth\")\n","plt.scatter(evaluated_x, evaluated_pred, s=3, c='orange', label=\"predicted\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"l1uLEg9DKGlP"},"source":["The neural network's training on the data resulted in a learned function, however, its performance is not optimal due to the limited representational power of a single layer. To overcome this, the next section will train a deeper neural network for improved fitting of the true function. But before that, let's first visualize this model using local linearization. Here is the first-order Talor expansion of the learned function $f$ w.r.t. the neural network parameters $\\mathbf{w}$:\n","\n","$$f(x,\\mathbf{w}') \\approx f(x,\\mathbf{w}) + \\langle \\nabla_\\mathbf{w} f(x, \\mathbf{w}) |_\\mathbf{w} , \\mathbf{w}' - \\mathbf{w}\\rangle$$\n","\n","Let's first visualize $\\nabla_\\mathbf{w} f(\\cdot, \\mathbf{w}) |_\\mathbf{w}$.\n","For each single $x$,\n","$\\nabla_\\mathbf{w} f(x, \\mathbf{w}) |_\\mathbf{w}$ is a vector in $\\mathbb{R}^m$,\n","where $m$ is the number of parameters of the neural network.\n","So given $n$ datapoints in the training dataset,\n","we stack these $m$-dim vectors together,\n","and get a matrix with size $n$ by $m$:\n","\n","$$\n","\\begin{pmatrix}f(x_1,\\mathbf{w}') \\\\ \\vdots \\\\ f(x_n,\\mathbf{w}')\\end{pmatrix} =\n","\\begin{pmatrix}f(x_1,\\mathbf{w}) \\\\ \\vdots \\\\ f(x_n,\\mathbf{w})\\end{pmatrix} +\n","\\begin{pmatrix}\\nabla_\\mathbf{w} f(x_1, \\mathbf{w}) |_\\mathbf{w}^T \\\\ \\vdots \\\\ \\nabla_\\mathbf{w} f(x_n, \\mathbf{w}) |_\\mathbf{w}^T\\end{pmatrix}(\\mathbf{w}' - \\mathbf{w})\n","$$\n","\n","\n","After that,\n","we can decompose this matrix with SVD:\n","\n","$$\n","\\begin{pmatrix}f(x_1,\\mathbf{w}') \\\\ \\vdots \\\\ f(x_n,\\mathbf{w}')\\end{pmatrix} =\n","\\begin{pmatrix}f(x_1,\\mathbf{w}) \\\\ \\vdots \\\\ f(x_n,\\mathbf{w})\\end{pmatrix} +\n","\\mathbf{U\\Sigma V}^T(\\mathbf{w}' - \\mathbf{w})\n","$$\n","\n","Notice that $\\mathbf{V}$ is an orthogonal matrix.\n","\n","Let's imagine that we have a generalized linear model with a feature matrix corresponding to the linearized features corresponding to each learnable parameter. The singular value and principal features of this matrix is important to us. Here is a visualization of singular values and principal features (rows of $\\mathbf{U\\Sigma})$:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q2PHk97OKGlQ"},"outputs":[],"source":["def svd_local_linearization(model):\n","    all_grads = []\n","    for idx in range(TRAIN_N):\n","        batch_x = torch.from_numpy(train_x[idx: idx + 1]).float().unsqueeze(-1)\n","        pred = model(batch_x).view(-1)\n","        model.zero_grad()\n","        pred.backward()\n","        flattened_grads = []\n","        for param in model.parameters():\n","            flattened_grads.append(param.grad.view(-1).data.numpy())\n","        flattened_grads = np.concatenate(flattened_grads)\n","        all_grads.append(flattened_grads)\n","    all_grads = np.stack(all_grads)\n","    u, s, vh = np.linalg.svd(all_grads, full_matrices=False)\n","    principal_feature = u * s\n","    return all_grads.shape[1], s, principal_feature\n","\n","\n","def visualize(m, singular_values, principal_feature):\n","    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14, 4))\n","    ax1.bar(np.arange(min(len(singular_values), 20)) + 1, singular_values[:20])\n","    ax1.set_title(\"Singular Values (Top-20)\")\n","\n","    for idx in range(min(m, 10)):\n","        ax2.scatter(train_x, principal_feature[:, idx], s=1,\n","                    label=(\"feature \" + str(idx + 1) if idx < 2 else None))\n","    ax2.set_title(\"Principal Features (Top-10)\")\n","    ax2.set_xlabel(\"x\", labelpad=0)\n","    ax2.set_ylabel(\"principal feature\", labelpad=0)\n","    ax2.legend()\n","\n","    im = ax3.scatter(principal_feature[:, 0], principal_feature[:, 1], c=train_x, s=2)\n","    ax3.set_title(\"Top-2 Features (x indicated by color)\")\n","    ax3.set_xlabel(\"feature 1\", labelpad=0)\n","    ax3.set_ylabel(\"feature 2\", labelpad=0)\n","    plt.colorbar(im, ax=ax3)\n","\n","    fig.tight_layout(pad=4)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tMjU83RrKGlQ"},"outputs":[],"source":["m, singular_values, principal_feature = svd_local_linearization(model)\n","visualize(m, singular_values, principal_feature)"]},{"cell_type":"markdown","metadata":{"id":"Hw0a7rp-KGlQ"},"source":["## Training a Deeper Neural Network\n","\n","In this section, we will train a deeper neural network with different weight scales for initialization. First, **define a fully-connected neural network with 4 hidden layers** and one ReLU activation after each layer except the last one, using the `nn.Sequential` API in PyTorch. Make sure that your model definition passes the assertions below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2CgjeRlYKGlQ"},"outputs":[],"source":["# Define a neural network with 4 hidden layers\n","# and ReLU activation after each layer except the output layer\n","\n","width = 20\n","\n","model = nn.Sequential(\n","    ####################################################################\n","    # TODO: YOUR CODE HERE\n","    ####################################################################\n","    NotImplementedError()\n","    ####################################################################\n",")\n","\n","assert len(model) == 9\n","assert sum(p.numel() for p in model.parameters()) == 1321\n","assert list(model(torch.randn(10, 1)).shape) == [10, 1]"]},{"cell_type":"markdown","metadata":{"id":"QR0gU3iEKGlR"},"source":["### When the weight scale is too small..."]},{"cell_type":"markdown","metadata":{"id":"4uUiYL5CKGlR"},"source":["The training of a deep neural network is highly impacted by the initialization of its parameters. Let's see what will happen if we initialize each entry in the weight matrices with random values drawn uniformly from the interval `[-0.03, 0.03]`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p4kIMNU-KGlR"},"outputs":[],"source":["# Initialize the neural network\n","\n","def naive_init(scale, module):\n","    if isinstance(module, nn.Linear):\n","        nn.init.uniform_(module.weight, -scale, scale)\n","        nn.init.zeros_(module.bias)\n","\n","\n","np.random.seed(200)\n","torch.manual_seed(200)\n","model.apply(functools.partial(naive_init, 0.03))"]},{"cell_type":"markdown","metadata":{"id":"ePruEiPNKGlR"},"source":["**Complete the code below to calculate and display the L2-norm of the gradients for each weight matrix**, based on a provided batch of training data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_0BDm5IKKGlR"},"outputs":[],"source":["def print_gnorms(model):\n","    batch_x = torch.from_numpy(train_x).float().unsqueeze(-1)\n","    batch_y = torch.from_numpy(train_y).float()\n","    pred = model(batch_x).view(-1)\n","    criterion = nn.MSELoss()\n","    loss = criterion(batch_y, pred)\n","    model.zero_grad()\n","    loss.backward()\n","\n","    for name, param in model.named_parameters():\n","        if name.endswith(\".weight\"):\n","            with torch.no_grad():\n","                ####################################################################\n","                # TODO: YOUR CODE HERE\n","                ####################################################################\n","                gnorm = ?\n","                ####################################################################\n","                print(name, \"{:.8f}\".format(gnorm))\n","\n","\n","print_gnorms(model)"]},{"cell_type":"markdown","metadata":{"id":"xMUiqLHvKGlS"},"source":["**Question: What are the gradient norms of each layer?** Copy the output of the last cell to your submission of the written assignment with your descriptions."]},{"cell_type":"markdown","metadata":{"id":"k_BxATTvKGlS"},"source":["Here is the visualization of principal features and singular values of local linearization of the neural network *before* the neural network is trained:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SnVeKQVQKGlS"},"outputs":[],"source":["m, singular_values, principal_feature = svd_local_linearization(model)\n","visualize(m, singular_values, principal_feature)"]},{"cell_type":"markdown","metadata":{"id":"oQ7Gsu4EKGlS"},"source":["If you find these figures difficult to read, printing out the values of the principal feature matrix may help you understand what happened:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1GsYvtRdKGlS"},"outputs":[],"source":["principal_feature"]},{"cell_type":"markdown","metadata":{"id":"fxsqwoWCKGlS"},"source":["Then let's try to train this model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bkf1h3nbKGlS"},"outputs":[],"source":["lr = 0.1\n","batch_size = 32\n","n_steps = int(TRAIN_N / batch_size * 10)\n","\n","model, evaluated_x, evaluated_pred = train_model(model, n_steps, batch_size, lr, 250)\n","\n","plt.figure()\n","plt.plot(test_x, f_true(test_x), label=\"truth\")\n","plt.scatter(evaluated_x, evaluated_pred, s=3, c='orange', label=\"predicted\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"9aJ0QzhIKGlS"},"source":["**Question: Describe the performance of this model.** Please include the answer in your submission of the written assignment."]},{"cell_type":"markdown","metadata":{"id":"s8KlG886KGlS"},"source":["Then let's visualize singular values and principal features of this model's local linearization after it has already been trained:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R1IdxrXYKGlS"},"outputs":[],"source":["m, singular_values, principal_feature = svd_local_linearization(model)\n","visualize(m, singular_values, principal_feature)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R1UkRINxKGlS"},"outputs":[],"source":["principal_feature"]},{"cell_type":"markdown","metadata":{"id":"52bk87zIKGlS"},"source":["**Question: Describe your observation of the principal features and singular values before and after training.** Please include the figures and your answer in your submission of the written assignment."]},{"cell_type":"markdown","metadata":{"id":"cVAEa4-LKGlT"},"source":["### When the weight scale is too large...\n","\n","Let's see what will happen if we instead initialize each entry in the weight matrices with random values drawn uniformly from the interval `[-3.0, 3.0]`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7jzRF3OGKGlT"},"outputs":[],"source":["np.random.seed(300)\n","torch.manual_seed(300)\n","model.apply(functools.partial(naive_init, 3.0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQfDpIzKKGlT"},"outputs":[],"source":["print_gnorms(model)"]},{"cell_type":"markdown","metadata":{"id":"p2d-AfxbKGlT"},"source":["**Question: What are the gradient norms of each layer?** Copy the output of the last cell to your submission of the written assignment with your descriptions."]},{"cell_type":"markdown","metadata":{"id":"zHuLz648KGlT"},"source":["Let's do some visualization before training:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W57D0T0zKGlT"},"outputs":[],"source":["m, singular_values, principal_feature = svd_local_linearization(model)\n","visualize(m, singular_values, principal_feature)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VMF9nAexKGlT"},"outputs":[],"source":["lr = 0.1\n","batch_size = 32\n","n_steps = int(TRAIN_N / batch_size * 10)\n","\n","model, evaluated_x, evaluated_pred = train_model(model, n_steps, batch_size, lr, 350)\n","\n","plt.figure()\n","plt.plot(test_x, f_true(test_x), label=\"truth\")\n","plt.scatter(evaluated_x, evaluated_pred, s=3, c='orange', label=\"predicted\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"QzV0wMFRKGlT"},"source":["**Question: What happened when we try to train this model?** Please include the answer in your submission of the written assignment."]},{"cell_type":"markdown","metadata":{"id":"-vlrAbGWKGlT"},"source":["### Better initialization methods\n","\n","**Implement a better initialization method** based on what you have learned on this course.\n","\n","HINT: a `module` with type `nn.Linear` has attributes `in_features` and `out_features`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MfNYAzmSKGlT"},"outputs":[],"source":["def better_init(module):\n","    if isinstance(module, nn.Linear):\n","        ########################################################################\n","        # TODO: YOUR CODE HERE\n","        ########################################################################\n","        raise NotImplementedError()\n","        ########################################################################\n","        nn.init.zeros_(module.bias)\n","\n","\n","np.random.seed(600)\n","torch.manual_seed(600)\n","model.apply(better_init)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m8mgsf-DKGlT"},"outputs":[],"source":["print_gnorms(model)"]},{"cell_type":"markdown","metadata":{"id":"XZcrNXv5KGlT"},"source":["**Question: What are the gradient norms of each layer?** Copy the output of the last cell to your submission of the written assignment with your descriptions."]},{"cell_type":"markdown","metadata":{"id":"pkFDRJ0tKGlT"},"source":["Let's visualize this freshly initialized model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-FZglnObKGlT"},"outputs":[],"source":["m, singular_values, principal_feature = svd_local_linearization(model)\n","visualize(m, singular_values, principal_feature)"]},{"cell_type":"markdown","metadata":{"id":"-R_7dLhvKGlT"},"source":["**Question: Compare what you have seen in these figures and compare it with the visualization of the model initialized with very large weight scales before training.** Please include the figures and your answer in your submission of the written assignment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sohe6IC0KGlT"},"outputs":[],"source":["lr = 0.1\n","batch_size = 32\n","n_steps = int(TRAIN_N / batch_size * 10)\n","\n","model, evaluated_x, evaluated_pred = train_model(model, n_steps, batch_size, lr, 650)\n","\n","plt.figure()\n","plt.plot(test_x, f_true(test_x), label=\"truth\")\n","plt.scatter(evaluated_x, evaluated_pred, s=3, c='orange', label=\"predicted\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"IP3S9cCqKGlT"},"source":["**Question: Describe the performance of this model.** Please include the answer in your submission of the written assignment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dWaOJcYaKGlT"},"outputs":[],"source":["m, singular_values, principal_feature = svd_local_linearization(model)\n","visualize(m, singular_values, principal_feature)"]},{"cell_type":"markdown","metadata":{"id":"weV1njA1KGlT"},"source":["**Question: Describe your observation and compare it with the visualization of the local linearization of neural network with a single hidden layer.** Please include the figures and your answer in your submission of the written assignment."]},{"cell_type":"markdown","metadata":{"id":"CvJAHdJbKGlU"},"source":["## Out-of-Distribution Generalization\n","\n","In this section, we investigate the impact of mismatched data distributions on the performance of the neural network by generating a new training set with $x$ values sampled from the interval -1.0 to 0.4."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bvtUr6vUKGlU"},"outputs":[],"source":["# Generate training data from -1 to 0.4\n","np.random.seed(50)\n","ood_train_x = np.random.uniform(low=-1.0, high=0.4, size=TRAIN_N)\n","ood_train_y = f_true(ood_train_x) + np.random.randn(TRAIN_N) * TRAIN_SIGMA\n","\n","plt.figure()\n","plt.scatter(ood_train_x, ood_train_y, s=3)\n","plt.xlim(-1.05, 1.05)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"poZjq77eKGlU"},"outputs":[],"source":["np.random.seed(600)\n","torch.manual_seed(600)\n","model.apply(better_init)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZEBMkyWoKGlU"},"outputs":[],"source":["lr = 0.1\n","batch_size = 32\n","n_steps = int(TRAIN_N / batch_size * 10)\n","\n","model, evaluated_x, evaluated_pred = train_model(model, n_steps, batch_size, lr, 650, ood_train_x, ood_train_y)\n","\n","plt.figure()\n","plt.plot(test_x, f_true(test_x), label=\"truth\")\n","plt.scatter(evaluated_x, evaluated_pred, s=3, c='orange', label=\"predicted\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"piPkU_77KGlU"},"source":["**Question: Describe the performance of this model.** Please include the answer in your submission of the written assignment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JuYfZfTPKGlU"},"outputs":[],"source":["m, singular_values, principal_feature = svd_local_linearization(model)\n","visualize(m, singular_values, principal_feature)"]},{"cell_type":"markdown","metadata":{"id":"w3q1z6XwKGlU"},"source":["**Question: Describe your observation and compare it with (a) the visualization of the local linearization of the neural network with a single hidden layer; (b) the visualization of the local linearization of the properly-initialized neural network with 4 hidden layers.** Please include the figures and your answer in your submission of the written assignment."]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.6 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.6"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}},"colab":{"provenance":[{"file_id":"18dkfXJ6WHMgO3mavD7q4dbPXjIk_-K9q","timestamp":1692845790132}]}},"nbformat":4,"nbformat_minor":0}