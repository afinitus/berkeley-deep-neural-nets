{"cells":[{"cell_type":"markdown","source":["# Setup Environment\n","\n","If you are working on this assignment using Google Colab, please execute the codes below.\n","\n","Alternatively, you can also do this assignment using a local anaconda environment (or a Python virtualenv). Please clone the GitHub repo by running `git clone https://github.com/Berkeley-CS182/cs182hw2.git` and refer to `README.md` for further details."],"metadata":{"id":"QAADrwaMdXah"}},{"cell_type":"code","source":["#@title Mount your Google Drive\n","\n","import os\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"FTqxsd2CdXal"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Set up mount symlink\n","\n","DRIVE_PATH = '/content/gdrive/My\\ Drive/cs182hw2_sp23'\n","DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n","if not os.path.exists(DRIVE_PYTHON_PATH):\n","  %mkdir $DRIVE_PATH\n","\n","## the space in `My Drive` causes some issues,\n","## make a symlink to avoid this\n","SYM_PATH = '/content/cs182hw2'\n","if not os.path.exists(SYM_PATH):\n","  !ln -s $DRIVE_PATH $SYM_PATH"],"metadata":{"id":"v4-ffeHIdXan"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Install dependencies\n","\n","!pip install numpy==1.21.6 imageio==2.9.0 matplotlib==3.2.2"],"metadata":{"id":"7MgSON7GdXan"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Clone homework repo\n","\n","%cd $SYM_PATH\n","if not os.path.exists(\"cs182hw2\"):\n","  !git clone https://github.com/Berkeley-CS182/cs182hw2.git\n","%cd cs182hw2"],"metadata":{"id":"yytW1gMIdXan"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Download datasets\n","\n","%cd deeplearning/datasets/\n","!bash ./get_datasets.sh\n","%cd ../.."],"metadata":{"id":"w9b_XgccdXao"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Configure Jupyter Notebook\n","\n","import matplotlib\n","%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2"],"metadata":{"id":"5xMSLoK8dXao"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y9BtneUjdXap"},"source":["# Optimization Methods and Initizalization\n","\n","Until now, you've always used Gradient Descent to update the parameters and minimize the cost. In this notebook, you will learn more advanced optimization methods that can speed up learning and perhaps even get you to a better final value for the cost function. Having a good optimization algorithm can be the difference between waiting days vs. just a few hours to get a good result.\n","\n","Gradient descent goes \"downhill\" on a cost function $J$. Think of it as trying to do this:\n","<img src=\"https://raw.githubusercontent.com/amanchadha/coursera-deep-learning-specialization/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%202/images/cost.jpg\">\n","<caption><center> <u> <strong>Figure 1</strong> </u>: <strong>Minimizing the cost is like finding the lowest point in a hilly landscape<strong/><br> At each step of the training, you update your parameters following a certain direction to try to get to the lowest possible point. </center></caption>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eX0i4ZttdXap"},"outputs":[],"source":["# As usual, a bit of setup\n","\n","import json\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from deeplearning.classifiers.fc_net import *\n","from deeplearning.data_utils import get_CIFAR10_data\n","from deeplearning.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n","from deeplearning.solver import Solver\n","import random\n","import torch\n","seed = 7\n","torch.manual_seed(seed)\n","random.seed(seed)\n","np.random.seed(seed)\n","\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","\n","def rel_error(x, y):\n","    \"\"\" returns relative error \"\"\"\n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OnGkH6kHdXaq"},"outputs":[],"source":["# Load the (preprocessed) CIFAR10 data.\n","\n","data = get_CIFAR10_data()\n","for k, v in data.items():\n","    print('%s: ' % k, v.shape)"]},{"cell_type":"markdown","metadata":{"id":"URBmoBV3dXaq"},"source":["## 1 - Stochastic Gradient Descent\n","\n","A simple optimization method in machine learning is gradient descent (GD). When you take gradient steps with respect to all $m$ examples on each step, it is also called Batch Gradient Descent.\n","\n","A variant of this is Stochastic Gradient Descent (SGD), which is equivalent to mini-batch gradient descent where each mini-batch has just 1 example. The update rule that you have just implemented does not change. What changes is that you would be computing gradients on just one training example at a time, rather than on the whole training set. The code examples below illustrate the difference between stochastic gradient descent and (batch) gradient descent.\n","\n","In Stochastic Gradient Descent, you use only 1 training example before updating the gradients. When the training set is large, SGD can be faster. But the parameters will \"oscillate\" toward the minimum rather than converge smoothly. Here is an illustration of this:\n","\n","<img src=\"https://raw.githubusercontent.com/amanchadha/coursera-deep-learning-specialization/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%202/images/kiank_sgd.png\">\n","<caption><center> <u>  <strong>Figure 1</strong> </u> : <strong>SGD vs GD</strong><br> \"+\" denotes a minimum of the cost. SGD leads to many oscillations to reach convergence. But each step is a lot faster to compute for SGD than for GD, as it uses only one training example (vs. the whole batch for GD). </center></caption>\n","\n","In the following code snippet, we will use SGD to optimze a five-layer FullyConnectedNet so that it overfits to 50 training exmaples."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"610_8XBvdXaq"},"outputs":[],"source":["## Use a five-layer Net to overfit 50 training examples.\n","\n","\n","num_train = 50\n","small_data = {\n","  'X_train': data['X_train'][:num_train],\n","  'y_train': data['y_train'][:num_train],\n","  'X_val': data['X_val'],\n","  'y_val': data['y_val'],\n","}\n","\n","weight_scale = 1e-1\n","learning_rate = 1e-3\n","model = FullyConnectedNet([100, 100, 100, 100],\n","                weight_scale=weight_scale, dtype=np.float64)\n","\n","solver = Solver(model, small_data,\n","                print_every=10, num_epochs=20, batch_size=25,\n","                update_rule='sgd',\n","                optim_config={\n","                  'learning_rate': learning_rate,\n","                }\n","         )\n","solver.train()\n","\n","plt.subplot(3, 1, 1)\n","plt.plot(solver.loss_history, 'o')\n","plt.title('Training loss history')\n","plt.xlabel('Iteration')\n","plt.ylabel('Training loss')\n","\n","plt.subplot(3, 1, 2)\n","plt.plot(solver.train_acc_history, 'o')\n","plt.title('Training Accuracy history')\n","plt.xlabel('Iteration')\n","plt.ylabel('Training Accuracy')\n","\n","plt.subplot(3, 1, 3)\n","plt.plot(solver.val_acc_history, 'o')\n","plt.title('Validation Accuracy history')\n","plt.xlabel('Iteration')\n","plt.ylabel('Validation Accuracy')\n","plt.gcf().set_size_inches(15, 15)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"fLuOUx7gdXar"},"source":["## 2 - Momentum\n","\n","Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will \"oscillate\" toward convergence. Using momentum can reduce these oscillations.\n","\n","Momentum takes into account the past gradients to smooth out the update. We will store the 'direction' of the previous gradients in the variable $v$. Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of $v$ as the \"velocity\" of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill.\n","\n","<img src=\"https://raw.githubusercontent.com/amanchadha/coursera-deep-learning-specialization/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%202/images/opt_momentum.png\">\n","<caption><center> <u><strong>Figure 3</strong></u>: The red arrows shows the direction taken by one step of mini-batch gradient descent with momentum. The blue points show the direction of the gradient (with respect to the current mini-batch) on each step. Rather than just following the gradient, we let the gradient influence v (velocity) and then take a step in the direction of v.<br> </center>\n","\n"," The momentum update rule for a weight matrix w is:\n","\n","$$ \\begin{cases}\n","v_{dw}^t = m * v_{dw}^{(t-1)} + dw\\\\\n","{w} = w - \\alpha  v_{dw}^t\n","\\end{cases}\\tag{3}$$\n","\n","\n","where $m$ is the momentum and $\\alpha$ is the learning rate. Note that the iterator `t` starts at 1.\n","\n","Stochastic gradient descent with momentum is a widely used update rule that tends to make deep networks converge faster than vanilla stochstic gradient descent, it can be viewed conceptually a larger \"effective batch size\" versus vanilla stochastic gradient descent.\n","\n","Open the file `deeplearning/optim.py` and read the documentation at the top of the file to make sure you understand the API. **Implement the SGD+momentum update rule** in the function `sgd_momentum` and run the following to check your implementation. You should see errors less than 1e-7."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nsay8GfpdXar"},"outputs":[],"source":["from deeplearning.optim import sgd_momentum\n","\n","N, D = 4, 5\n","w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n","dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n","v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n","\n","config = {'learning_rate': 1e-3, 'velocity': v}\n","next_w, _ = sgd_momentum(w, dw, config=config)\n","\n","expected_next_w = np.asarray([\n"," [-0.39994, -0.347375263, -0.294810526, -0.242245789, -0.189681053],\n"," [-0.137116316, -0.084551579, -0.031986842, 0.020577895, 0.073142632],\n"," [0.125707368, 0.178272105, 0.230836842, 0.283401579, 0.335966316],\n"," [0.388531053, 0.441095789, 0.493660526, 0.546225263, 0.59879]])\n","expected_velocity = np.asarray([\n"," [-0.06, 0.006842105, 0.073684211, 0.140526316, 0.207368421],\n"," [0.274210526, 0.341052632, 0.407894737, 0.474736842, 0.541578947],\n"," [0.608421053, 0.675263158, 0.742105263, 0.808947368, 0.875789474],\n"," [0.942631579, 1.009473684, 1.076315789, 1.143157895, 1.21]\n","])\n","\n","print ('next_w error: ', rel_error(next_w, expected_next_w))\n","print ('velocity error: ', rel_error(expected_velocity, config['velocity']))"]},{"cell_type":"markdown","metadata":{"id":"KdyLV7bcdXar"},"source":["Once you have done so, run the following to train a six-layer network with both SGD and SGD+momentum. You should see the SGD+momentum update rule converge a bit faster."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4GC93_HvdXar"},"outputs":[],"source":["num_train = 4000\n","small_data = {\n","  'X_train': data['X_train'][:num_train],\n","  'y_train': data['y_train'][:num_train],\n","  'X_val': data['X_val'],\n","  'y_val': data['y_val'],\n","}\n","\n","solvers = {}\n","\n","for update_rule in ['sgd', 'sgd_momentum']:\n","    print ('running with ', update_rule)\n","    model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n","\n","    solver = Solver(model, small_data,\n","                  num_epochs=5, batch_size=100,\n","                  update_rule=update_rule,\n","                  optim_config={\n","                    'learning_rate': 1e-2,\n","                  },\n","                  verbose=True)\n","    solvers[update_rule] = solver\n","    solver.train()\n","    os.makedirs(\"submission_logs\", exist_ok=True)\n","    solver.record_histories_as_npz(\"submission_logs/optimizer_experiment_{}\".format(update_rule))\n","    print\n","\n","plt.subplot(3, 1, 1)\n","plt.title('Training loss')\n","plt.xlabel('Iteration')\n","\n","plt.subplot(3, 1, 2)\n","plt.title('Training accuracy')\n","plt.xlabel('Epoch')\n","\n","plt.subplot(3, 1, 3)\n","plt.title('Validation accuracy')\n","plt.xlabel('Epoch')\n","\n","for update_rule, solver in solvers.items():\n","    plt.subplot(3, 1, 1)\n","    plt.plot(solver.loss_history, 'o', label=update_rule)\n","\n","    plt.subplot(3, 1, 2)\n","    plt.plot(solver.train_acc_history, '-o', label=update_rule)\n","\n","    plt.subplot(3, 1, 3)\n","    plt.plot(solver.val_acc_history, '-o', label=update_rule)\n","\n","for i in [1, 2, 3]:\n","    plt.subplot(3, 1, i)\n","    plt.legend(loc='upper center', ncol=4)\n","plt.gcf().set_size_inches(15, 15)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"6CdTxrRZdXar"},"source":["A further step: as we discussed above, we can see how SGD+Momentum is conceptually giving you a larger \"effective batch size\" by increase the batch size used in the SGD above. In this way, SGD+Momentum can significantly speed up training.\n","\n","**Tune the batch size for plain SGD** so that the training accuracy is similar to that of SGD with momentum. The average accuracy difference between them should be less than `0.04`. The accuracy is averaged over three different random seeds for better stability."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8PxXiOcdXas"},"outputs":[],"source":["#############################################################################\n","# TODO: Tune the batch size for the SGD below until you observe             #\n","# similar end of iteration training performance.                            #\n","# It means rel_error(train_acc) < 0.04                                      #\n","#############################################################################\n","batch_sizes = {\n","  'sgd_momentum': 100,\n","  'sgd': ?,  # tune the batch size of SGD (must be multiples of 100)\n","}\n","\n","num_train = 6000\n","small_data = {\n","  'X_train': data['X_train'][:num_train],\n","  'y_train': data['y_train'][:num_train],\n","  'X_val': data['X_val'],\n","  'y_val': data['y_val'],\n","}\n","\n","solvers = {}\n","total_acc = {}\n","\n","labels = {\n","  'sgd_momentum': 'sgd_momentum',\n","  'sgd': 'sgd_large_bsz',\n","}\n","\n","for update_rule in ['sgd', 'sgd_momentum']:\n","    print ('running with', update_rule, ' ; seed =', seed)\n","    # set the epochs so that we have the same number of steps for both rules\n","    training_epochs = 5 * int(batch_sizes[update_rule]/100)\n","    solvers[update_rule] = {}\n","    total_acc[update_rule] = 0\n","\n","    for seed in [100, 200, 300]:\n","        torch.manual_seed(seed)\n","        np.random.seed(seed)\n","        model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n","\n","        solver = Solver(\n","            model, small_data,\n","            num_epochs=training_epochs,\n","            batch_size=batch_sizes[update_rule],\n","            update_rule=update_rule,\n","            optim_config={\n","                'learning_rate': 1e-2,  # please do not change the learning rate\n","            },\n","            verbose=True,\n","            log_acc_iteration=True)\n","\n","        solvers[update_rule][seed] = solver\n","        solver.train()\n","        solver.record_histories_as_npz(\n","            \"submission_logs/sgd_momentum_compare_{}_{}\"\n","            .format(update_rule, seed)\n","        )\n","\n","        total_acc[update_rule] += solvers[update_rule][seed].train_acc_history[-1]\n","\n","print('Average Training Acc for sgd:', total_acc['sgd'] / 3)\n","print('Average Training Acc for sgd_momentum:', total_acc['sgd_momentum'] / 3)\n","print('Train Acc Difference: ',\n","       rel_error(total_acc['sgd'] / 3,\n","                 total_acc['sgd_momentum'] / 3))\n","\n","def plot_solver_seeds(solver_s, x_field, y_field, seeds, label):\n","    a = np.array([getattr(solver_s[seed], y_field) for seed in seeds])\n","    if x_field is None:\n","        plt_x = np.arange(a.shape[1]) + 1\n","    else:\n","        plt_x = getattr(solver_s[seeds[0]], x_field)\n","    plt.plot(plt_x, a.mean(axis=0), label=label)\n","    plt.fill_between(plt_x, a.min(axis=0), a.max(axis=0), alpha=0.4)\n","\n","plt.subplot(3, 1, 1)\n","plt.title('Training loss')\n","plt.xlabel('Iteration')\n","\n","plt.subplot(3, 1, 2)\n","plt.title('Training accuracy')\n","plt.xlabel('Iteration')\n","\n","plt.subplot(3, 1, 3)\n","plt.title('Validation accuracy')\n","plt.xlabel('Iteration')\n","\n","for update_rule, solver_s in solvers.items():\n","    plt.subplot(3, 1, 1)\n","    # plt.plot(solver.loss_history, 'o', label=labels[update_rule])\n","    plot_solver_seeds(solver_s, None, 'loss_history',\n","                      [100, 200, 300], labels[update_rule])\n","\n","    plt.subplot(3, 1, 2)\n","    # plt.plot(solver.log_acc_iteration_history, solver.train_acc_history, '-o', label=labels[update_rule])\n","    plot_solver_seeds(solver_s, 'log_acc_iteration_history', 'train_acc_history',\n","                      [100, 200, 300], labels[update_rule])\n","\n","    plt.subplot(3, 1, 3)\n","    # plt.plot(solver.log_acc_iteration_history, solver.val_acc_history, '-o', label=labels[update_rule])\n","    plot_solver_seeds(solver_s, 'log_acc_iteration_history', 'val_acc_history',\n","                      [100, 200, 300], labels[update_rule])\n","\n","for i in [1, 2, 3]:\n","    plt.subplot(3, 1, i)\n","    plt.legend(loc='upper center', ncol=4)\n","plt.gcf().set_size_inches(15, 15)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"OeIubAA9dXas"},"source":["\n","## 3 - Adam\n","\n","Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp and Momentum.\n","\n","<strong>How does Adam work?<strong>\n","1. It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $m^{corrected}$ (with bias correction).\n","2. It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $v^{corrected}$ (with bias correction).\n","3. It updates parameters in a direction based on combining information from \"1\" and \"2\".\n","\n","$$\\begin{cases}\n","m_{dw} = \\beta_1 m_{dw} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W } \\\\\n","m^{corrected}_{dw} = \\frac{m_{dw}}{1 - (\\beta_1)^t} \\\\\n","v_{dw} = \\beta_2 v_{dw} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W })^2 \\\\\n","v^{corrected}_{dw} = \\frac{v_{dw}}{1 - (\\beta_2)^t} \\\\\n","w = w - \\alpha \\frac{m^{corrected}_{dw}}{\\sqrt{v^{corrected}_{dw}} + \\varepsilon}\n","\\end{cases}$$\n","where:\n","- t counts the number of steps taken of Adam\n","- $\\beta_1$ and $\\beta_2$ are hyperparameters that control the two exponentially weighted averages.\n","- $\\alpha$ is the learning rate\n","- $\\varepsilon$ is a very small number to avoid dividing by zero\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FXx56MyFdXas"},"source":["\n","RMSProp [1] and Adam [2] are update rules that set per-parameter learning rates by using a running average of the second moments of gradients.\n","\n","In the file `deeplearning/optim.py`, **implement the RMSProp update rule** in the `rmsprop` function (optional, the solution is provided at the bottom of optim.py) and **implement the Adam update rule** in the `adam` function, and check your implementations using the tests below.\n","\n","[1] Tijmen Tieleman and Geoffrey Hinton. \"Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.\" COURSERA: Neural Networks for Machine Learning 4 (2012).\n","\n","[2] Diederik Kingma and Jimmy Ba, \"Adam: A Method for Stochastic Optimization\", ICLR 2015."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iqT2CtjidXas"},"outputs":[],"source":["# Test RMSProp implementation; you should see errors less than 1e-7.\n","from deeplearning.optim import rmsprop\n","\n","N, D = 4, 5\n","w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n","dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n","cache = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n","\n","config = {'learning_rate': 1e-2, 'cache': cache}\n","next_w, _ = rmsprop(w, dw, config=config)\n","\n","expected_next_w = np.asarray([\n","  [-0.39223849, -0.34037513, -0.28849239, -0.23659121, -0.18467247],\n","  [-0.132737,   -0.08078555, -0.02881884,  0.02316247,  0.07515774],\n","  [ 0.12716641,  0.17918792,  0.23122175,  0.28326742,  0.33532447],\n","  [ 0.38739248,  0.43947102,  0.49155973,  0.54365823,  0.59576619]])\n","expected_cache = np.asarray([\n","  [ 0.5976,      0.6126277,   0.6277108,   0.64284931,  0.65804321],\n","  [ 0.67329252,  0.68859723,  0.70395734,  0.71937285,  0.73484377],\n","  [ 0.75037008,  0.7659518,   0.78158892,  0.79728144,  0.81302936],\n","  [ 0.82883269,  0.84469141,  0.86060554,  0.87657507,  0.8926    ]])\n","\n","print ('next_w error: ', rel_error(expected_next_w, next_w))\n","print ('cache error: ', rel_error(expected_cache, config['cache']))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSE9BLlhdXas"},"outputs":[],"source":["# Test Adam implementation; you should see errors around 1e-7 or less.\n","from deeplearning.optim import adam\n","\n","N, D = 4, 5\n","w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n","dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n","m = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n","v = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)\n","\n","config = {'learning_rate': 1e-2, 'm': m, 'v': v, 't': 5}\n","next_w, _ = adam(w, dw, config=config)\n","\n","expected_next_w = np.asarray([\n","  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n","  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n","  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n","  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])\n","expected_v = np.asarray([\n","  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n","  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n","  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n","  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]])\n","expected_m = np.asarray([\n","  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n","  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n","  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n","  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])\n","expected_t = 6\n","\n","print ('next_w error: ', rel_error(expected_next_w, next_w))\n","print ('v error: ', rel_error(expected_v, config['v']))\n","print ('m error: ', rel_error(expected_m, config['m']))\n","print ('t error: ', rel_error(expected_t, config['t']))"]},{"cell_type":"markdown","metadata":{"id":"HVdoeLHydXat"},"source":["Once you have debugged your RMSProp and Adam implementations, run the following to train a pair of deep networks using these new update rules. As a sanity check, you should see that RMSProp and Adam typically obtain at least 45% training accuracy within 5 epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EwHkePKadXat"},"outputs":[],"source":["num_train = 4000\n","small_data = {\n","  'X_train': data['X_train'][:num_train],\n","  'y_train': data['y_train'][:num_train],\n","  'X_val': data['X_val'],\n","  'y_val': data['y_val'],\n","}\n","\n","learning_rates = {'rmsprop': 1e-4, 'adam': 1e-3, 'sgd': 1e-2, 'sgd_momentum': 1e-2}\n","for update_rule in ['sgd', 'sgd_momentum', 'adam', 'rmsprop']:\n","    print ('running with ', update_rule)\n","\n","    torch.manual_seed(0)\n","    np.random.seed(0)\n","\n","    model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n","\n","    solver = Solver(model, small_data,\n","                  num_epochs=5, batch_size=100,\n","                  update_rule=update_rule,\n","                  optim_config={\n","                    'learning_rate': learning_rates[update_rule]\n","                  },\n","                  verbose=True,)\n","    solvers[update_rule] = solver\n","    solver.train()\n","    solver.record_histories_as_npz(\"submission_logs/optimizer_experiment_{}\".format(update_rule))\n","    print\n","\n","plt.subplot(3, 1, 1)\n","plt.title('Training loss')\n","plt.xlabel('Iteration')\n","\n","plt.subplot(3, 1, 2)\n","plt.title('Training accuracy')\n","plt.xlabel('Epoch')\n","\n","plt.subplot(3, 1, 3)\n","plt.title('Validation accuracy')\n","plt.xlabel('Epoch')\n","\n","for update_rule, solver in solvers.items():\n","    plt.subplot(3, 1, 1)\n","    plt.plot(solver.loss_history, label=update_rule)\n","\n","    plt.subplot(3, 1, 2)\n","    plt.plot(solver.train_acc_history, '-o', label=update_rule)\n","\n","    plt.subplot(3, 1, 3)\n","    plt.plot(solver.val_acc_history, '-o', label=update_rule)\n","\n","for i in [1, 2, 3]:\n","    plt.subplot(3, 1, i)\n","    plt.legend(loc='upper center', ncol=4)\n","plt.gcf().set_size_inches(15, 15)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Aa7HQINDdXat"},"source":["# Initialization\n","\n","Training your neural network requires specifying an initial value of the weights. A well chosen initialization method will help learning.  \n","\n","\n","A well chosen initialization can:\n","- Speed up the convergence of gradient descent\n","- Increase the odds of gradient descent converging to a lower training (and generalization) error\n","\n","We will use three different initilization methods to illustrate this concept.\n","\n","- Zero Initialization:\n","\n","    This initializes the weights to 0.\n","\n","\n","- Random Initialization:\n","\n","    This initializes the weights drawn from a distribution with *manually* specified scales. In this homework, **we use normal distribution with the `weight_scale` argument in `fc_net.py` as its std.**\n","\n","- He/Xavier/Glorot Initialization:\n","\n","    This is a special case for random initialization, where the scaling factor is set so that the std of each parameter is `gain / sqrt(fan_mode)`. `gain` is determined by the activation function. For example, linear activation has `gain = 1` and ReLU activation has `gain = sqrt(2)`. There are three types of fan mode:\n","    - Fan in: `fan_mode = in_dim`, i.e., the width of the preceding layer, preserving the magnitude in forward pass. **This is what you need to implement below** and also the default in PyTorch.\n","    - Fan out: `fan_mode = out_dim`, i.e., the width of the succeeding layer, preserving the magnitude in backpropagation.\n","    - Average: `fan_mode = (in_dim + out_dim) / 2`.\n","\n","    When the std is determined, another choice is between normal distribution or uniform distribution. In this homework **we use normal distribution for initialization.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Sy3VUSVdXat"},"outputs":[],"source":["#############################################################################\n","# TODO:\n","# 1. implement three initialization schemes in\n","#    deeplearning/classifiers/fc_net.py\n","# 2. record the mean of l2 norm of the gradients\n","#    in the deeplearning/solver.py\n","#############################################################################\n","\n","learning_rates = {'sgd': 1e-3}\n","update_rule = 'sgd'\n","solvers = dict()\n","\n","num_train = 4000\n","small_data = {\n","  'X_train': data['X_train'][:num_train],\n","  'y_train': data['y_train'][:num_train],\n","  'X_val': data['X_val'],\n","  'y_val': data['y_val'],\n","}\n","\n","for initialization in ['he', 'random', 'zero']:\n","    print ('running with ', update_rule)\n","\n","    model = FullyConnectedNet([50]*10, initialization=initialization)\n","    weight_stds = [float(model.params[\"W\" + str(i)].std()) for i in range(1, 12)]\n","    print(\"initialization scheme:\", initialization)\n","    if initialization == \"he\":\n","        # It is fine if the rel_error is less than 0.03 due to randomness\n","        print(\"Layer 1, rel_error\", rel_error(0.02551551815399, weight_stds[0]))\n","        print(\"Layer 2, rel_error\", rel_error(0.2, weight_stds[1]))\n","    elif initialization == \"random\":\n","        # It is fine if the rel_error is less than 0.03 due to randomness\n","        print(\"Layer 1, rel_error\", rel_error(0.01, weight_stds[0]))\n","        print(\"Layer 2, rel_error\", rel_error(0.01, weight_stds[1]))\n","    with open(\"submission_logs/w_stds_{}.json\".format(initialization), \"w\", encoding=\"utf-8\") as f:\n","        json.dump(weight_stds, f)\n","\n","    solver = Solver(model, small_data,\n","                  num_epochs=5, batch_size=100,\n","                  update_rule=update_rule,\n","                  optim_config={\n","                    'learning_rate': learning_rates[update_rule]\n","                  },\n","                  verbose=True)\n","    solvers[initialization] = solver\n","    solver.train()\n","    solver.record_histories_as_npz(\"submission_logs/initialization_experiment_{}\".format(initialization))\n","    print\n","\n","plt.subplot(4, 1, 1)\n","plt.title('Training loss')\n","plt.xlabel('Iteration')\n","\n","\n","plt.subplot(4, 1, 2)\n","plt.title('Training accuracy')\n","plt.xlabel('Epoch')\n","\n","plt.subplot(4, 1, 3)\n","plt.title('Validation accuracy')\n","plt.xlabel('Epoch')\n","\n","plt.subplot(4, 1, 4)\n","plt.title('Mean of the Gradient Norm')\n","plt.xlabel('Iteration')\n","\n","for initialization, solver in solvers.items():\n","    plt.subplot(4, 1, 1)\n","    plt.plot(solver.loss_history, label=initialization)\n","\n","    plt.subplot(4, 1, 2)\n","    plt.plot(solver.train_acc_history, '-o', label=initialization)\n","\n","    plt.subplot(4, 1, 3)\n","    plt.plot(solver.val_acc_history, '-o', label=initialization)\n","\n","    plt.subplot(4, 1, 4)\n","    plt.plot(solver.log_grad_norm_history, label=initialization)\n","\n","for i in [1, 2, 3, 4]:\n","    plt.subplot(4, 1, i)\n","    plt.legend(loc='upper center', ncol=4)\n","plt.gcf().set_size_inches(15, 20)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"perJfaCddXat"},"source":["### Question:\n","\n","**What you observe in the mean of gradient norm plot above in the above plots?** Try to give an explanation. **Write your answer on the written assignment.**"]},{"cell_type":"markdown","metadata":{"id":"44omU1HadXat"},"source":["# Train a good model!\n","Train the best fully-connected model that you can on CIFAR-10, storing your best model in the `best_model` variable and the solver used in the `best_solver` variable. We require you to get at least 45% accuracy *on the validation set* using a fully-connected net.\n","\n","If you are careful it should be possible to get accuracies above 55%, but we don't require it for this part and won't assign extra credit for doing so. Later in the assignment we will ask you to train the best convolutional network that you can on CIFAR-10, and we would prefer that you spend your effort working on convolutional nets rather than fully-connected nets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d0m9ywb4dXat"},"outputs":[],"source":["best_model = None\n","best_solver = None\n","\n","\n","width = 200  # please don't change this\n","n_layers = 10  # please don't change this\n","\n","################################################################################\n","# TODO: Train the best FullyConnectedNet that you can on CIFAR-10.             #\n","# Store your best model in the best_model variable                             #\n","# and the solver used to train it in the best_solver variable                  #\n","# Please use the He Initialization and adam.                                   #\n","# You could tune the following variables only below,                           #\n","# it shoud achieve above 45% accuracy on the validation set.                   #\n","################################################################################\n","lr = ?\n","num_epochs = ?\n","batch_size = ?\n","lr_decay = ?\n","update_rule = ?\n","################################################################################\n","#                              END OF YOUR CODE                                #\n","################################################################################\n","\n","np.random.seed(2023)  # please don't change this for reproducibility\n","torch.manual_seed(2023)  # please don't change this for reproducibility\n","model = FullyConnectedNet([width] * n_layers,\n","                          initialization='he'\n","                          )\n","solver = Solver(model,\n","                data,\n","                num_epochs=num_epochs,\n","                batch_size=batch_size,\n","                update_rule=update_rule,\n","                optim_config={\n","                  'learning_rate': lr\n","                },\n","                lr_decay=lr_decay,\n","                verbose=True)\n","solver.train()\n","best_model = model\n","best_solver = solver"]},{"cell_type":"markdown","metadata":{"id":"jEqX-hBTdXau"},"source":["# Test your model\n","Run your best model on the validation and test sets and record the training logs of the best solver. You should achieve above 45% accuracy on the validation set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZZLPfSzbdXau"},"outputs":[],"source":["y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n","y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n","val_acc = (y_val_pred == data['y_val']).mean()\n","test_acc = (y_test_pred == data['y_test']).mean()\n","print ('Validation set accuracy: ', val_acc)\n","print ('Test set accuracy: ', test_acc)\n","best_solver.record_histories_as_npz('submission_logs/best_fc_model.npz')\n","import json\n","with open(\"submission_logs/results.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(dict(\n","        val_acc = val_acc,\n","        test_acc = test_acc,\n","        lr = lr,\n","        num_epochs = num_epochs,\n","        batch_size = batch_size,\n","        lr_decay = lr_decay,\n","        update_rule = update_rule\n","    ), f)"]},{"cell_type":"markdown","source":["# Collect your submissions\n","\n","On Colab, after running the following cell, you can download your submissions from the `Files` tab, which can be opened by clicking the file icon on the left hand side of the screen."],"metadata":{"id":"XMDqGIAYdXau"}},{"cell_type":"code","source":["!rm -f cs182hw2_submission.zip\n","!zip -r cs182hw2_submission.zip . -x \"*.git*\" \"*deeplearning/datasets*\" \"*.ipynb_checkpoints*\" \"*README.md\" \".env/*\" \"*.pyc\" \"*deeplearning/build/*\" \"*__pycache__/*\""],"metadata":{"id":"_dcYz17vdXau"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/Berkeley-CS182/cs182hw2/blob/main/hw2_optimizer_init.ipynb","timestamp":1692845785688}]}},"nbformat":4,"nbformat_minor":0}