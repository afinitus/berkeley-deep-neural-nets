{"cells":[{"cell_type":"markdown","metadata":{"id":"X2sWcYziBrUj"},"source":["# Memory considerations when training Neural Networks on GPUs\n","\n","In this homework, we will train a ResNet model on CIFAR-10 using PyTorch and explore it's implications on GPU memory.\n","\n","We will explore various systems considerations, such as the effect of batch size\n","on memory usage, the effect of different optimizers (SGD, SGD with momentum, Adam), and we will try to minimize the memory usage of training our model by applying gradient accumulation.\n"]},{"cell_type":"markdown","metadata":{"id":"UVeTO39_LoOW"},"source":["## Setup the environment"]},{"cell_type":"markdown","metadata":{"id":"gOIdwf6aO2A4"},"source":["If you're running on colab - make sure you are using a GPU runtime. You can select a GPU runtime by clicking on `Runtime` -> `Change Runtime Type`.\n","\n","> ðŸ’¡ Hint - if you hit your colab GPU usage limit, try again in a few hours."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ey9qkAfLMgm2"},"outputs":[],"source":["#@title Mount your Google Drive\n","\n","import os\n","from google.colab import drive\n","\n","try:\n","  drive.mount('/content/gdrive')\n","\n","  DRIVE_PATH = '/content/gdrive/My\\ Drive/cs182hw4_sp23'\n","  DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n","  if not os.path.exists(DRIVE_PYTHON_PATH):\n","    %mkdir $DRIVE_PATH\n","\n","  ## the space in `My Drive` causes some issues,\n","  ## make a symlink to avoid this\n","  SYM_PATH = '/content/cs182hw4'\n","  if os.path.isdir(SYM_PATH):\n","    raise Exception(f\"Path already exists - please delete {SYM_PATH} before mounting again.\")\n","  else:\n","    !ln -sf $DRIVE_PATH $SYM_PATH\n","except Exception as e:\n","  print(e)\n","  print(\"WARNING - Unable to mount google drive for storing logs. Storing logs in the runtime. Please keep downloading the logs to avoid losing data if the runtime terminates.\")\n","  os.makedirs('/content/cs182hw4/', exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QUQyoFYBLS3H"},"outputs":[],"source":["#@title Install dependencies\n","\n","!pip install gputil"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"leJM2aNIOtRH"},"outputs":[],"source":["import gc\n","import GPUtil\n","import os\n","import subprocess\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import random\n","import time\n","\n","\n","ROOT_PATH = '/content/cs182hw4/'\n","\n","# Define the CSV format for logging memory usage. Used later in this notebook.\n","MEMORY_LOG_FMT = ['timestamp', 'memUsage']\n","TRAIN_LOG_FMT = ['timestamp', 'epoch', 'memUsage', 'loss', 'accuracy']\n","\n","if torch.cuda.is_available():\n","  print(\"Using GPU.\")\n","  device = torch.device(\"cuda:0\")\n","else:\n","  print(\"!!! WARNING !!! - Could not find a GPU - please use a GPU for this homework! If you're on Colab, change your runtime to GPU.\")\n","  device = torch.device(\"cpu\")\n","\n","%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"akM0PvICPfQz"},"source":["### Define helper functions and download CIFAR-10 dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dGbpzrznPvqi"},"outputs":[],"source":["seed = 42\n","torch.manual_seed(seed)\n","random.seed(seed)\n","np.random.seed(seed)\n","\n","def get_allocated_memory_str():\n","    return \"Allocated memory: {:.2f} GB\".format(torch.cuda.memory_allocated(device) / 1e9)\n","\n","def run_nvidia_smi():\n","    if torch.cuda.is_available():\n","      print(subprocess.check_output(\"nvidia-smi\", shell=True).decode(\"utf-8\"))\n","    else:\n","      print(\"Running on CPU\")\n","\n","def get_gpu_memory_usage() -> float:\n","    # Use GPUtil python library to get GPU memory usage\n","    if torch.cuda.is_available():\n","      return GPUtil.getGPUs()[0].memoryUsed\n","    else:\n","      return 0\n","\n","def cleanup_memory():\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","# Define transformations for the input data. We resize the 32x32 inputs to\n","# 224x224 which is the input shape for the ResNet family of models.\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)), # Resize to 224x224 for ResNet models\n","    transforms.ToTensor()\n","])\n","\n","data_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","data_test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","\n","\n","# We randomly subsample the dataset here to train our models faster for this notebook\n","SUBSAMPLE_SIZE = 1024*4\n","random_sample_idxs = torch.randint(len(data_train), (SUBSAMPLE_SIZE,))\n","subsampled_train_data = torch.utils.data.Subset(data_train, indices=random_sample_idxs)"]},{"cell_type":"markdown","metadata":{"id":"0LI96hOtTbGR"},"source":["# 1. Managing GPU memory when training deep models\n","One of the most common bottlenecks you will run into when training your deep learning models is the amount of GPU memory available to you. The exact memory usage of your training process depends on the specific model architecture and the size of the input data. The main components taking up GPU memory during training are:\n","\n","* **Model Parameters**: The weights and biases of the model are stored in GPU memory during training. The number of parameters in a deep learning model can range from a few thousand to millions or even billions, depending on the model architecture and the size of the input data.\n","\n","* **Activations**: The activations of each layer of the model are stored in GPU memory during the forward pass. The size of the activations can depend on the batch size and the number of hidden units in each layer. As the batch size increases, so does the size of the activations, which can quickly consume a large amount of GPU memory.\n","\n","* **Gradients**: During the backward pass, the gradients of each layer with respect to the loss function are computed and stored in GPU memory. The size of the gradients can depend on the batch size and the number of hidden units in each layer. Like activations, larger batch sizes can lead to larger gradients and increased memory usage.\n","\n","* **Input Data**: The input data, such as images or text, can also take up GPU memory during training. The size of the input data can depend on the input shape and the batch size.\n","\n","* **Optimizer State**: The state of the optimizer, such as the momentum or running average of gradients, is stored in GPU memory during training. The size of the optimizer state can depend on the optimizer algorithm and the size of the model parameters.\n","\n","## Let's analyze the ResNet-152 model and CIFAR-10 input sizes\n","\n","We can count the number of parameters in the model by loading it and inspecting it. Once we know the number of parameters, we can estimate the model size by multiplying the number of parameters with the size of each parameter."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yBCjqoeaYxgw"},"outputs":[],"source":["def analyze_model_and_inputs(model):\n","    print(\"Train data size: {}\".format(len(data_train)))\n","    print(\"Test data size: {}\".format(len(data_test)))\n","\n","    # Fetch an example image to get image size\n","    image, label = data_train[0]\n","    print(\"Image input size: {}\".format(image.size()))\n","\n","    # Get model parameter count\n","    print(\"Model parameters: {}\".format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n","\n","    # Get model size in MB\n","    print(\"Model size estimate (MB): {}\".format(sum(p.numel() * p.element_size() for p in model.parameters()) / 1e6))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"229oW1WnDRxT"},"outputs":[],"source":["model = torchvision.models.resnet152(weights=None, num_classes=10)\n","model.to(device)  # Load the model into GPU memory\n","analyze_model_and_inputs(model)"]},{"cell_type":"markdown","metadata":{"id":"jtP0AZToaRL-"},"source":["## Let's get to know our GPU better\n","\n","Now that we have loaded the model onto the GPU, we will now use the `nvidia-smi` utility to measure the GPU memory utilization."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5aUg0RgDaV92"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"yXTxNxbZBkig"},"source":["Note that the actual memory usage on the GPU is anywhere between ~500-1000 MB larger than the model size computed above. Why? In addition to loading the model, the GPU also needs to be initialized with essential kernels, memory allocation tables, and other GPU related state necessary to using the GPU. This is called the CUDA context.\n","\n","The CUDA context can be considered a fixed memory overhead for using a Nvidia GPU."]},{"cell_type":"markdown","metadata":{"id":"ogi7Sxelae8x"},"source":["# Questions (answer in written submission)\n","**Q1a. How many trainable parameters does ResNet-152 have? What is the estimated size of the model in MB?**\n","\n","**Q1b. Which GPU are you using? How much total memory does it have?**\n","\n","**Q1c. After you load the model into memory, what is the memory overhead (size) of the CUDA context loaded with the model?**\n","\n","> Hint - CUDA context size in this example is roughly (total GPU memory utilization - model size)"]},{"cell_type":"markdown","metadata":{"id":"4G_iw8Sju0NX"},"source":["# 2. Optimizer memory usage\n","\n","The choice of optimizer affects the memory used to train your model. Different optimizers have different memory requirements for storing the gradients and the optimizer state. For example, the Adam optimizer stores a moving average of the gradients and the squared gradients for each parameter, which requires more memory than SGD.\n","\n","Let's compare the memory usage of three different optimizers - SGD, SGD with momentum and ADAM.\n","\n","Below we define a simple function to profile the memory usage for a given optimizer.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eyqCye0yu8mX"},"outputs":[],"source":["# Training function\n","def train_model(model, train_loader, criterion, optimizer, epochs=10, memory_log_path=None):\n","    os.makedirs(os.path.dirname(memory_log_path), exist_ok=True)\n","    with open(memory_log_path, 'w') as f:\n","      f.write(\",\".join(MEMORY_LOG_FMT) + \"\\n\")\n","    for epoch in range(epochs):\n","        model.train()\n","        for i, (images, labels) in enumerate(train_loader):\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            with torch.set_grad_enabled(True):\n","              # Zero all gradients\n","              optimizer.zero_grad()\n","\n","              # Get outputs\n","              outputs = model(images)\n","\n","              # Compute loss\n","              loss = criterion(outputs, labels)\n","              loss.backward()\n","\n","              # Run optimizer update step\n","              optimizer.step()\n","\n","              # Print stats every 100 iterations\n","              if i % 100 == 0:\n","                  gpu_memory_usage = get_gpu_memory_usage()\n","                  print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, GPU Mem: {}'.format(epoch+1, epochs, i+1, len(train_loader), loss.item(), gpu_memory_usage))\n","                  memory_log = [str(time.time()), str(gpu_memory_usage)]\n","                  with open(memory_log_path, 'a') as f:\n","                    f.write(\",\".join(memory_log) + \"\\n\")\n","            del loss, outputs, images, labels  # To get accurate memory usage info\n","\n","# Memory profiling function\n","def profile_mem_usage(optimizer_str):\n","    \"\"\"\n","    Profiles the memory usage of ResNet-152 on CIFAR-10 with the specified optimizer.\n","\n","    optimizer_str: str - Can be either of 'SGD', 'SGD_WITH_MOMENTUM' and 'ADAM'\n","    \"\"\"\n","    # Clean up any dangling objects\n","    cleanup_memory()\n","    BATCH_SIZE = 8\n","\n","    # Since we just want to inspect memory usage, run only one minibatch\n","    subsampled_data = torch.utils.data.Subset(data_train, range(0, BATCH_SIZE))\n","    train_loader = torch.utils.data.DataLoader(dataset=subsampled_data,\n","                                               batch_size=BATCH_SIZE,\n","                                               shuffle=True)\n","\n","    # Load model and define loss function\n","    model = torchvision.models.resnet152(weights=None, num_classes=10)\n","    model.to(device)\n","    criterion = torch.nn.CrossEntropyLoss()\n","\n","    # Choose optimizer\n","    if optimizer_str == 'SGD':\n","        optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n","    elif optimizer_str == 'SGD_WITH_MOMENTUM':\n","        optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","    elif optimizer_str == 'ADAM':\n","        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","    else:\n","        raise NotImplementedError\n","\n","    memory_log_path = ROOT_PATH + f'logs/resnet152__{optimizer_str}.csv'\n","    train_model(model, train_loader, criterion, optimizer, epochs=1, memory_log_path=memory_log_path)\n","    print(f\"Memory usage log for {optimizer_str} stored at {memory_log_path}. Restart your runtime (Runtime->Restart Runtime) before running for other optimizers!\")"]},{"cell_type":"markdown","metadata":{"id":"505p5gyMjDax"},"source":["## Run memory profiling for various optimizers!\n","\n","In the cell below, run the `profile_mem_usage` method with three optimizers -\n","`'SGD', 'SGD_WITH_MOMENTUM', 'ADAM'`.\n","\n","ðŸŽ‡ NOTE ðŸŽ‡ - to get accurate memory utilization measurements, you <u>**should restart your runtime between invoking `profile_mem_usage` for different optimizers!**</u>\n","\n","There is state in GPU memory that is not collect by explicitly calling the garbage collector, and thus restarting the runtime is necessary. Your files in colab should persist across runs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LHpW0sPL_LTW"},"outputs":[],"source":["# TODO - Run this cell for different optimizers by uncommenting one line at a time.\n","#\n","# Make sure to restart the colab runtime between different runs else your\n","# memory profiles may be inaccurate!\n","\n","\n","profile_mem_usage('SGD')\n","# profile_mem_usage('SGD_WITH_MOMENTUM')\n","# profile_mem_usage('ADAM')"]},{"cell_type":"markdown","metadata":{"id":"ysB9xqLriq4M"},"source":["## Analyzing memory usage profiles\n","\n","Now that you have run `profile_mem_usage` for different optimizers, let's print the memory usage we logged while training with each optimizer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EkSQ6WSdcrvs"},"outputs":[],"source":["OPTIMIZER_LIST = ['SGD', 'SGD_WITH_MOMENTUM', 'ADAM']\n","memory_log_path = ROOT_PATH + 'logs/resnet152__{opt}.csv'\n","\n","def print_mem_profiling_results():\n","    print(\"====== Memory Profiling Results ======\")\n","    for opt in OPTIMIZER_LIST:\n","        assert os.path.exists(memory_log_path.format(opt=opt)), f'Memory profile not found for {opt}. Have you run profile_mem_usage({opt}) in the above cell?'\n","        df = pd.read_csv(memory_log_path.format(opt=opt))\n","        mem_usage = df['memUsage'].iloc[0]\n","        print(f'{opt}: {mem_usage} MB')\n","\n","print_mem_profiling_results()"]},{"cell_type":"markdown","metadata":{"id":"_RgLrnKzu6RM"},"source":["\n","## Questions (answer in written submission)\n","**2a. What is the total memory utilization during training with SGD, SGD with momentum and Adam optimizers?** Report in MB individually for each optimizer.\n","\n","**2b. Which optimizer consumes the most memory? Why?**\n","\n","> ðŸ’¡ Hint - refer to the weight update rule for each optimizer. Which one requires the most parameters to be stored in memory?"]},{"cell_type":"markdown","metadata":{"id":"J3AOR_YfWnbW"},"source":["# 3. Investigating the effect of batch size on convergence and GPU memory\n","Batch size is an important parameter in training neural networks that can have a significant effect on GPU memory usage. The larger the batch size, the more data the model processes at once, and therefore, the more GPU memory it requires to store the inputs, activations, and gradients.\n","\n","As the batch size increases, the memory required to store the intermediate results during training increases linearly. This is because the model needs to keep track of more activations and gradients for each layer. However, the actual memory usage can also depend on the specific neural network architecture, as some models require more memory than others to process the same batch size.\n","\n","If the batch size is too large to fit in the available GPU memory, the training process will fail with an out-of-memory error. On the other hand, if the batch size is too small, the training may be slower due to inefficient use of the GPU, as the GPU may spend more time waiting for data to be transferred from CPU to GPU.\n","\n","Therefore, choosing an appropriate batch size is important to balance training speed and memory usage. This often involves some trial and error to find the largest batch size that can fit in the available GPU memory while still providing good training results.\n","\n","## Learning Rate and Batch Size\n","\n","Batch size and learning rate are closely related. When batch size is increased, the gradient estimate becomes less noisy because it is computed over more samples. As a result, the learning rate can be increased, allowing the optimization algorithm to take larger steps towards the optimum. This is because a larger batch size gives a more accurate estimate of the direction of the gradient and larger steps can reduce convergence time.\n","\n","Large batch training becomes particularly important in data-parallel distributed training, where extremely large batch sizes are distributed over many GPUs. The paper [\"Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour\"](https://arxiv.org/pdf/1706.02677.pdf) is one of the earliest works showing how large batch training makes fast large scale distributed training possible. It also proposes a simple linear scaling rule for setting the learning rate for a given batch size, which we use to set learning rates in `LR_MAP` below.\n","\n","## Let's try training our model with different batch sizes\n","In the below cells, we'll try to run training for different batch sizes and evaluate the performance.\n","\n","> Note - you may run out of memory for large batch sizes, and that is expected! Ignore those large batch sizes and stick with the batch sizes that can fit on your GPU.\n","\n","Let's first define helper functions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUTeD0X9a2zA"},"outputs":[],"source":["# Test function\n","def test_model(model, test_loader, label='test'):\n","    print(\"Testing model.\")\n","    model.eval()\n","    with torch.no_grad():\n","        correct = 0\n","        total = 0\n","\n","        for images, labels in test_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            outputs = model(images)\n","\n","            _, predicted = torch.max(outputs.data, 1)\n","\n","            correct += (predicted == labels).sum().item()\n","            total += labels.size(0)\n","\n","        # Compute accuracy\n","        accuracy = 100 * correct / total\n","        print(f'Accuracy of the model on {label} images: {accuracy} %')\n","        del outputs, images, labels  # To get accurate memory usage info\n","    return accuracy\n","\n","# Training function\n","def train_model(model, train_loader, criterion, optimizer, epochs=10, memory_log_path=None, test_loader = None):\n","    os.makedirs(os.path.dirname(memory_log_path), exist_ok=True)\n","    with open(memory_log_path, 'w') as f:\n","      f.write(\",\".join(TRAIN_LOG_FMT) + \"\\n\")\n","    for epoch in range(epochs):\n","        model.train()\n","        last_loss = 0\n","        for i, (images, labels) in enumerate(train_loader):\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            with torch.set_grad_enabled(True):\n","              # Zero all gradients\n","              optimizer.zero_grad()\n","\n","              # Get outputs\n","              outputs = model(images)\n","\n","              # Compute loss\n","              loss = criterion(outputs, labels)\n","              loss.backward()\n","\n","              # Run optimizer update step\n","              optimizer.step()\n","\n","              last_loss = loss.item()\n","              # Print stats every 100 iterations\n","              if i % 10 == 0:\n","                  gpu_memory_usage = get_gpu_memory_usage()\n","                  print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, GPU Mem: {}'.format(epoch+1, epochs, i+1, len(train_loader), last_loss, gpu_memory_usage))\n","            del loss, outputs, images, labels  # To get accurate memory usage info\n","\n","        # Report test or train accuracy at the end of every epoch\n","        if test_loader:\n","          accuracy = test_model(model, test_loader, label='test')\n","        else:\n","          accuracy = test_model(model, train_loader, label='train')\n","\n","        # Log results\n","        memory_log = [str(time.time()), str(epoch+1), str(gpu_memory_usage), str(last_loss), str(accuracy)]\n","        with open(memory_log_path, 'a') as f:\n","            f.write(\",\".join(memory_log) + \"\\n\")\n","\n","# Set learning rates for different batch sizes (emperically determined and linearly scaled)\n","LR_MAP = {\n","    4: 0.0001,\n","    8: 0.0002,\n","    16: 0.0004,\n","    32: 0.0008,\n","    64: 0.0016,\n","    128: 0.0032,\n","    256: 0.0064,\n","    512: 0.0064,\n","    1024: 0.0064\n","}\n","\n","# Executor function\n","def run_train(batch_size, epochs=10):\n","    cleanup_memory()\n","\n","    lr = LR_MAP[batch_size]\n","    print(f\"Training model with batch size {batch_size} and lr {lr}.\")\n","\n","    train_loader = torch.utils.data.DataLoader(dataset=subsampled_train_data, batch_size=batch_size, shuffle=True)\n","\n","    # We use a smaller model (resnet18) to train faster\n","    model = torchvision.models.resnet18(weights=None, num_classes=10)\n","    model.to(device)\n","    criterion = torch.nn.CrossEntropyLoss()\n","\n","    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n","\n","    # Output path for memory logs\n","    memory_log_path = ROOT_PATH + f'logs/resnet18__{batch_size}.csv'\n","\n","    # Run training!\n","    train_model(model, train_loader, criterion, optimizer, epochs=epochs, memory_log_path=memory_log_path)"]},{"cell_type":"markdown","metadata":{"id":"jO45WqejxC8q"},"source":["## Run training for different batch sizes and record their memory utilization!\n","\n","In the cell below, **run the `run_train` method for batch sizes 4, 16, 64, 256, 1024**.\n","\n","This method will log the loss, accuracy, wall clock time and memory utilization under `/content/cs182hw4/logs` directory, so you can safely restart the runtime between invocations.\n","\n","Like before, to get accurate memory utilization measurements, you should <u>**restart your runtime between invoking `run_train` for different batch sizes!**</u>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"E1ruoh1X5bD3"},"outputs":[],"source":["# TODO - Run this cell for different batch sizes by uncommenting one line at a time.\n","#\n","# Make sure to restart the colab runtime between different runs else your\n","# memory profiles may be inaccurate!\n","\n","# Run each batch size for at least 10 epochs. You can configure this to be larger if you like.\n","epochs = 10\n","\n","run_train(4, epochs=epochs)\n","# run_train(16, epochs=epochs)\n","# run_train(64, epochs=epochs)\n","# run_train(256, epochs=epochs)\n","# run_train(1024, epochs=epochs)"]},{"cell_type":"markdown","metadata":{"id":"LEo8m_84yj2d"},"source":["## Plot the loss, accuracy and memory utilization\n","\n","Once all logs have been generated under `/content/cs182hw4/logs`, run the cell below to plot loss and accuracy against wall clock time."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Q-dj9-xjHb7C"},"outputs":[],"source":["# Plotting scripts\n","def get_df(batch_size):\n","    path = ROOT_PATH + f'logs/resnet18__{batch_size}.csv'\n","    assert os.path.exists(path), f'Memory profile not found for batch size {batch_size}. Have you run run_train({batch_size}) in the above cell?'\n","    df = pd.read_csv(path)\n","    # Create a wall time column\n","    df['walltime'] = df['timestamp'] - df['timestamp'].iloc[0]\n","    return df\n","\n","def plot_walltime_acc(batch_sizes):\n","    plt.figure()\n","    for bs in batch_sizes:\n","        df=get_df(bs)\n","        plt.plot(df['walltime'], df['accuracy'], label=f'batch_size: {bs}')\n","    plt.xlabel('Wall clock time (s)')\n","    plt.ylabel('Accuracy (%)')\n","    plt.legend()\n","    plt.title('Train Accuracy vs Walltime')\n","    plt.show()\n","\n","def print_mem_usage(batch_sizes):\n","    print(\"\\n====== Memory Usage for different batch sizes =======\")\n","    for bs in batch_sizes:\n","        df=get_df(bs)\n","        mem_usage = df['memUsage'].iloc[-1]\n","        print(f'{bs}\\t: {mem_usage} MB')\n","\n","batch_sizes = [4, 16, 64, 256]\n","plot_walltime_acc(batch_sizes)\n","print_mem_usage(batch_sizes)"]},{"cell_type":"markdown","metadata":{"id":"gkBHFlSf0zKE"},"source":["## Questions (answer in written submission)\n","**3a. What is the memory utilization for different batch sizes (4, 16, 64, 256)? What is the largest batch size you were able to train?**\n","\n","**3b. Which batch size gave you the highest accuracy at the end of 10 epochs?**\n","\n","**3c. Which batch size completed 10 epochs the fastest (least wall clock time)? Why?**\n","\n","**3d. Attach your training accuracy vs wall time plots with your written  submission.**\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1zOUxc4lwCml72cUWWk6nXHmPQVWDrRH5","timestamp":1692845681730}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}