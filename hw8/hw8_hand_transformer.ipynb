{"cells":[{"cell_type":"markdown","metadata":{"id":"j-q9zgiEy1e7"},"source":["# Implement Transformer from Scratch\n","\n","In this coding homework, you will:\n","\n","- Implement a simple transformer model from scratch to enhance your understanding of how it works.\n","- Create a hand-designed transformer model capable of solving a basic problem. This will help you comprehend the various operations that transformers can perform.\n","- Analyze the attention patterns of a trained network to gain insights into how learned models often utilize features that differ greatly from those employed by humans.\n","\n","Please note that a GPU is not necessary for this task. If you're using Colab, you can select the \"Runtime\" -> \"Change runtime type\" menu and choose \"None\" as the hardware accelerator.\n","\n","**Note:** The same variables will be defined in different ways in various subparts of the homework. If you encounter errors stating that a variable has the wrong shape or a function is missing an argument, ensure that you have re-run the cells in that particular problem subpart."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xp4QTxnoy1e9"},"outputs":[],"source":["import time\n","import json\n","import inspect\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import math\n","import random\n","import matplotlib.pyplot as plt\n","plt.rcParams['figure.figsize'] = [20, 5]  # Adjust this to make plots bigger or smaller\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","def _set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","\n","TO_SAVE = {\"time\": time.time()}"]},{"cell_type":"markdown","metadata":{"id":"SwuAsBRry1e-"},"source":["## Implement a Simple Transformer\n","\n","Below, you'll find a simple transformer implementation in Numpy that we have provided for you. It's important to note that this implementation is different from a Transformer in real applications. The differences include:\n","\n","- Only a single layer with a single head is in the network.\n","- There are no residual connections.\n","- There is no normalization or dropout.\n","- We concatenate the positional encoding rather than adding it to the inputs.\n","- There are no activation functions or MLP layers.\n","- It does not support attention masking.\n","- The input is a single sequence instead of a batch. So there is no need to implement padding.\n","\n","To ensure that you understand the transformer model fully, your task is to **implement a PyTorch equivalent model**. You don't need to include the printing and plotting code found in the Numpy version. **You should implement a vectorized version of the attention operation**, meaning that you should calculate all attention scores at once, rather than looping over keys. Once you have completed your implementation, make sure it passes the tests included in the cell below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ZWNkCEBy1e-"},"outputs":[],"source":["#@title Helper Functions\n","\n","def rescale_and_plot(arr, title='', ax=None, x_lab=None, y_lab=None):\n","    \"\"\"Rescale input array to be between 0 and 1, then plot it\"\"\"\n","    arr = (arr - arr.min())\n","    if arr.max() > 0:\n","        arr = arr / arr.max()\n","    ax.imshow(arr, cmap=\"Reds\")\n","    ax.set_title(title)\n","    ax.set_xticks([])\n","    ax.set_yticks([])\n","    if x_lab is not None:\n","        ax.set_xlabel(x_lab)\n","    if y_lab is not None:\n","        ax.set_ylabel(y_lab)\n","\n","\n","def train_loop(make_batch, input_dim, qk_dim, v_dim, pos_dim=None, max_seq_len=None, remove_cls=False, num_epochs=10001, lr=3e-2):\n","    transformer = PytorchTransformer(input_dim, qk_dim, v_dim, pos_dim, max_seq_len)\n","    optimizer = torch.optim.SGD(transformer.parameters(), lr=lr)\n","    loss_fn = nn.MSELoss()\n","    for i in range(num_epochs):\n","        seq, target = make_batch()\n","        optimizer.zero_grad()\n","        out = transformer(seq)\n","        # If remove_cls is True, remove the first item of the sequence (the CLS token)\n","        if remove_cls:\n","            out = out[1:]\n","        loss = loss_fn(out, target)\n","        loss.backward()\n","        optimizer.step()\n","        if i % 1000 == 0:\n","            print(f'Step {i}: loss {loss.item()}')\n","    return transformer, loss.item()\n","\n","def compare_transformers(hand_transformer, learned_transformer, seq):\n","    # Print the learned matrices\n","    # Rescale each weight matrix to be between 0 and 1, then plot them\n","    print('=' * 40, ' Hand Designed ', '=' * 40)\n","    out_hand = hand_transformer.forward(seq, verbose=False, plot=True)\n","\n","    # Copy weights from the learned transformer to the hand transformer\n","    # so we can run the hand transformer's forward pass, with the plotting code\n","    py_Km = learned_transformer.Km.weight.T.detach().numpy()\n","    py_Qm = learned_transformer.Qm.weight.T.detach().numpy()\n","    py_Vm = learned_transformer.Vm.weight.T.detach().numpy()\n","    # positional encodings, if they exist\n","    if learned_transformer.pos is not None:\n","        py_pos = learned_transformer.pos.weight.detach().numpy()\n","    else:\n","        py_pos = None\n","\n","    print('=' * 40, '   Learned   ', '=' * 40)\n","    np_learned_transformer = NumpyTransformer(py_Km, py_Qm, py_Vm, py_pos)\n","    out_learned = np_learned_transformer.forward(seq, verbose=False, plot=True)\n","    return out_hand, out_learned\n","\n","# Test the numpy transformer and pytorch transformer to make sure they give the same results\n","def test():\n","    min_seq_len = 1\n","    max_seq_len = 4\n","    qk_dim = np.random.randint(1, 5)\n","    v_dim = np.random.randint(1, 5)\n","    in_dim = 5\n","    for i in range(10):\n","        # Randomly sample the matrices\n","        Km = np.random.randn(in_dim, qk_dim)\n","        Qm = np.random.randn(in_dim, qk_dim)\n","        Vm = np.random.randn(in_dim, v_dim)\n","        if i > 4:\n","            # Sometimes, don't use positional encodings\n","            pos = pos_dim = None\n","            seq_dim = in_dim\n","        else:\n","            pos_dim = np.random.randint(2, 4)\n","            pos = np.random.randn(max_seq_len, pos_dim)\n","            seq_dim = in_dim - pos_dim\n","\n","        # Randomly sample the sequence\n","        seq = np.random.randn(np.random.randint(min_seq_len, max_seq_len + 1), seq_dim)\n","        # Get the numpy transformer output\n","        out_np = NumpyTransformer(Km, Qm, Vm, pos).forward(seq, verbose=False)\n","        # Create a pytorch transformer and fill the weights with the numpy matrices\n","        transformer = PytorchTransformer(seq_dim, qk_dim, v_dim, pos_dim, max_seq_len)\n","        state_dict = transformer.state_dict()\n","        # Replace the weights with the numpy matrices\n","        state_dict['Km.weight'] = torch.FloatTensor(Km.T)\n","        state_dict['Qm.weight'] = torch.FloatTensor(Qm.T)\n","        state_dict['Vm.weight'] = torch.FloatTensor(Vm.T)\n","        if pos is not None:\n","            state_dict['pos.weight'] = torch.FloatTensor(pos)\n","        transformer.load_state_dict(state_dict)\n","        # Get the pytorch transformer output\n","        out_py = transformer(torch.FloatTensor(seq)).detach().numpy()\n","        # Compare the outputs\n","        if not np.allclose(out_np, out_py, rtol=1e-3):\n","            print('ERROR!!')\n","            print('Numpy output', out_np)\n","            print('Pytorch output', out_py)\n","            print('Difference', out_np - out_py)\n","            raise ValueError('Numpy and Pytorch outputs do not match')\n","    print('All done!')\n","    _set_seed(1998)\n","    transformer = PytorchTransformer(7, 4, 3, 2, 9)\n","    o = transformer(torch.randn(8, 7))\n","    TO_SAVE[\"torch_transformer_shape\"] = list(o.shape)\n","    TO_SAVE[\"torch_transformer_value\"] = o.view(-1).tolist()[2:7]\n","    TO_SAVE[\"torch_transformer_init\"] = inspect.getsource(PytorchTransformer.__init__)\n","    TO_SAVE[\"torch_transformer_forward\"] = inspect.getsource(PytorchTransformer.forward)"]},{"cell_type":"markdown","source":["Implement the `PytorchTransformer` class. It should be identical to the forward pass of the `NumpyTransformer` class.\n","\n","**Hint:** The attention operation should be implemented as:\n","\n","$$\\mathrm{softmax}(\\dfrac{QK^T}{ \\sqrt{d_k}}) \\cdot V$$\n","\n","where the softmax is applied to the last dimension, meaning that the softmax is applied independently to each query's scores."],"metadata":{"id":"HOXPqZDBy1e_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"elO3aBWny1e_"},"outputs":[],"source":["#@title Numpy Transformer and PyTorch Transformer\n","\n","class NumpyTransformer:\n","    def __init__(self, Km, Qm, Vm, pos=None):\n","        \"\"\"\n","        # Km, Qm, Vm are the matrices that will be used to compute the attention\n","        # Km and Qm are size (input_dim + pos_dim, qk_dim), and Vm is (input_dim + pos_dim, v_dim).\n","        # pos is an array of positional encodings of shape (max_seq_len, pos_dim) that will be concatenated to the input sequence\n","        \"\"\"\n","        self.Km = Km\n","        self.Qm = Qm\n","        self.Vm = Vm\n","        self.pos = pos\n","        self.qk_dim = Qm.shape[1]\n","\n","    def forward(self, seq, verbose=False, plot=False):\n","        # seq is a numpy array of shape (seq_len, input_dim). There is no batch dimension.\n","\n","        # Concatenate positional encodings if they are provided\n","        if self.pos is not None:\n","            seq = np.concatenate([seq, self.pos[:seq.shape[0]]], axis=-1)\n","        K = seq @ self.Km # seq_len x qk_dim\n","        Q = seq @ self.Qm # seq_len x qk_dim\n","        V = seq @ self.Vm # seq_len x v_dim\n","        if verbose:\n","            print('Keys', K.tolist())\n","            print('Queries', Q.tolist())\n","            print('Values', V.tolist())\n","        if plot:\n","            fig, axs = plt.subplots(nrows=1,ncols=8)\n","            fig.tight_layout()\n","            rescale_and_plot(self.Km.T, 'Km', axs[0], x_lab='d_i', y_lab='d_qk')\n","            rescale_and_plot(self.Qm.T, 'Qm', axs[1], x_lab='d_i', y_lab='d_qk')\n","            rescale_and_plot(self.Vm.T, 'Vm', axs[2], x_lab='d_i', y_lab='d_v')\n","\n","            rescale_and_plot(K.T, 'K', axs[3], x_lab='seq', y_lab='d_qk')\n","            rescale_and_plot(Q.T, 'Q', axs[4], x_lab='seq', y_lab='d_qk')\n","            rescale_and_plot(V.T, 'V', axs[5], x_lab='seq', y_lab='d_v')\n","\n","        outputs = []\n","        attn_weights = []\n","        # Compute attention\n","        for i, q in enumerate(Q):\n","            if verbose: print(f'Item {i}: Computing attention for query {q}')\n","            dot = K @ q\n","            if verbose: print('  Dot products between the query and each key:', dot)\n","            # Divide by sqrt(qk_dim)\n","            dot = dot / np.sqrt(self.qk_dim)\n","            # Softmax function\n","            softmax_dot = np.exp(dot) / np.sum(np.exp(dot), axis=-1, keepdims=True)\n","            if verbose: print('  Weighting score for each value:', softmax_dot)\n","            attn_weights.append(softmax_dot)\n","            out_i = softmax_dot @ V\n","            if verbose: print('  New sequence item', out_i)\n","            outputs.append(out_i)\n","        if plot:\n","            rescale_and_plot(np.array(attn_weights).T, 'Attn', axs[6], x_lab='Q', y_lab='K')\n","            rescale_and_plot(np.array(outputs).T, 'Out', axs[7], x_lab='seq', y_lab='d_v')\n","            plt.show()\n","\n","        # Return the output sequence (seq_len, output_dim)\n","        return np.array(outputs)\n","\n","class PytorchTransformer(nn.Module):\n","    def __init__(self, input_dim, qk_dim, v_dim, pos_dim=None, max_seq_len=10):\n","        super().__init__()\n","        if pos_dim is not None:\n","            self.pos = nn.Embedding(max_seq_len, pos_dim)\n","        else:\n","            self.pos = None\n","        in_dim = input_dim\n","        if pos_dim is not None:\n","            in_dim += pos_dim\n","\n","        ########################################################################\n","        # TODO: Define query, key, value projection layers Qm, Km, Vm.\n","        #       Each of them is a linear projection without bias\n","        ########################################################################\n","        self.Qm = NotImplementedError()\n","        self.Km = NotImplementedError()\n","        self.Vm = NotImplementedError()\n","        ########################################################################\n","\n","        self.d_k = qk_dim\n","\n","    def forward(self, seq):\n","        \"\"\"\n","        Transformer forward pass\n","\n","        Inputs: seq is a torch tensor of shape (seq_len, input_dim).\n","        Outputs: a torch tensor of shape (seq_len, v_dim), the output of the attention operation\n","        \"\"\"\n","        ################################################################################################\n","        # TODO: Implement the forward pass of the `PytorchTransformer` class.\n","        #       The forward pass should be identical to the forward pass of the\n","        #       `NumpyTransformer` class.\n","        #\n","        # Hint: The attention operation should be implemented as\n","        #       If `pos` exists, it should be concatenated to the input sequence.\n","        #################################################################################################\n","        if self.pos is not None:\n","            seq = NotImplementedError()\n","\n","        # Compute attention\n","        out = NotImplementedError()\n","        ################################################################################################\n","        # END OF YOUR CODE\n","        ################################################################################################\n","        return out\n","\n","test()"]},{"cell_type":"markdown","metadata":{"id":"m0pDL6qpy1fA"},"source":["## Self-Attention: Attention by Content\n","\n","In this coding homework, we will explore how Transformers can attend to different tokens in a variable-length sequence based on their contents. We will do this by **implementing a Transformer that performs the *identity* operation on a sequence of one-hot vectors**. We will then compare the performance and weights of this hand-coded Transformer with those of a PyTorch model trained on the same task.\n","\n","To hand-design the Transformer, we will **choose values for `Km`, `Qm`, and `Vm` that enable the model to attend to the content of each token in the input sequence**. We will then use this Transformer to process several example data points, and verify that the output matches the input.\n","\n","Once your hand-written Transformer is working correctly, we will run the PyTorch training loop to train a model on the identity operation task. We will then compare the weights and intermediate outputs of this model with those of our hand-coded transformer, and comment on their similarities and differences. Note that when we generate plots, we will rescale the range of the weights and outputs to 0-1, so we can compare their relative values without comparing absolute values.\n","\n","The test cases for our hand-coded transformer are as follows:\n","\n","```\n","Input sequence -->   Output sequence\n","[A, B, C, C]   -->   [A, B, C, C]\n","[C, A, C]      -->   [C, A, C]\n","[B, B, C]      -->   [B, B, C]\n","```\n","\n","We have provided some hints below, but to enhance your understanding of attention and the Transformer, we highly recommend attempting this problem to the best of your abilities before referring to the hints."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Sb_W9Fwy1fA","cellView":"form"},"outputs":[],"source":["#@title Hints\n","\n","# Hint 1: To attend to a specific element, ensure that its pre-softmax score is\n","#         significantly higher than that of the other elements.\n","softmax = lambda x: np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n","print('='*20, 'Hint 1', '='*20)\n","print('Selecting index 0', softmax(np.array([9, 0, 0])))\n","print('Selecting index 1', softmax(np.array([-3, 5, -5])))\n","\n","\n","# Hint 2: Attending to a particular element is more manageable if the keys are\n","#         orthogonal.\n","print('='*20, 'Hint 2', '='*20)\n","keys = np.array([[2, 0], [0, 1]])  # Orthogonal\n","q = np.array([5, 0])\n","print('Selecting index 0', softmax(q @ keys))\n","q = np.array([0, 5])\n","print('Selecting index 1', softmax(q @ keys))\n","\n","\n","# Hint 3: You can use the following helper functions to test the keys, queries,\n","#         and values produced by your matrix for each valid sequence element.\n","#  Km, Qm, Vm, and are the matrices you will define below.\n","all_token_seq = np.eye(3)  # Each row is a sequence element. The identity corresponds to [A, B, C].\n","get_K = lambda: all_token_seq @ Km  # Each row of the output is a key\n","get_Q = lambda: all_token_seq @ Qm  # Each row of the output is a query\n","get_V = lambda: all_token_seq @ Vm  # Each row of the output is a value\n","\n","\n","# Hint 4: To test different attention weights, use the softmax function defined\n","#         above.\n","\n","\n","# Hint 5: When there are repeated elements in a sequence with the same content,\n","#         attending to all of them rather than a single one will be simpler.\n","#         Since they have the same content, taking a \"weighted average\" over\n","#         values weighted by attention scores will produce the same output as\n","#         attending to a single one."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iSJTwPREy1fB"},"outputs":[],"source":["# The definition of tokens\n","A = np.array([1,0,0])\n","B = np.array([0,1,0])\n","C = np.array([0,0,1])\n","tokens = [A, B, C]\n","\n","################################################################################\n","# TODO: Write Numpy arrays for `Km`, `Qm`, and `Vm`.\n","#       The dimensions should be (input_dim, qk_dim), (input_dim, qk_dim), and\n","#       (input_dim, v_dim), respectively.\n","#       In this case, input_dim = 3, and v_dim = 3. qk_dim can be any value you\n","#       choose, but 3 is a reasonable choice.\n","################################################################################\n","Km = NotImplementedError()\n","Qm = NotImplementedError()\n","Vm = NotImplementedError()\n","############################################ END OF YOUR CODE ##################\n","\n","def generate_test_cases_identity(tokens, max_len=7):\n","    \"\"\"\n","    Generate a random sequence consisting of tokens for testing\n","    \"\"\"\n","    seq_len = np.random.randint(1, max_len)\n","    input_arr = np.stack(random.choices(tokens, k=seq_len))\n","    expected_out = input_arr\n","    return input_arr, expected_out\n","\n","# Test your implementation\n","show_attention = False  # Set this to True for debugging\n","for i in range(10):\n","    seq, expected_out = generate_test_cases_identity(tokens)\n","    np_transformer = NumpyTransformer(Km, Qm, Vm)\n","    out = np_transformer.forward(seq, verbose=show_attention)\n","    if not np.allclose(out, expected_out, rtol=1e-3):\n","        print(f'FAIL: {seq} -> {out} != {expected_out}')\n","\n","_set_seed(1997)\n","seq, _ = generate_test_cases_identity(tokens)\n","np_transformer = NumpyTransformer(Km, Qm, Vm)\n","out = np_transformer.forward(seq, verbose=False)\n","TO_SAVE[\"attention_by_content\"] = out.reshape(-1).tolist()\n","TO_SAVE[\"attention_by_content_Q\"] = Qm.reshape(-1).tolist()\n","TO_SAVE[\"attention_by_content_K\"] = Km.reshape(-1).tolist()\n","TO_SAVE[\"attention_by_content_V\"] = Vm.reshape(-1).tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tO6QkOh3y1fB"},"outputs":[],"source":["# Compare the hand-designed and trained transformers\n","def make_batch_identity(tokens=tokens, max_len=7):\n","    seq, target = generate_test_cases_identity(tokens, max_len=max_len)\n","    return torch.FloatTensor(seq), torch.FloatTensor(target)\n","\n","_set_seed(227)\n","\n","A = np.array([1,0,0])\n","B = np.array([0,1,0])\n","C = np.array([0,0,1])\n","transformer_py, loss = train_loop(make_batch_identity, input_dim=len(A), qk_dim=Km.shape[1], v_dim=Vm.shape[1])\n","seq = np.stack([A, B, B, C, C])\n","print(\"seq:\", seq)\n","compare_transformers(np_transformer, transformer_py, seq)  # If the plots don't print correctly, re-run this cell"]},{"cell_type":"markdown","source":["### Question\n","\n","In the figure provided, compare the variables of your hand-designed Transformer with those of the learned Transformer. **Identify the similarities and differences between the two sets of variables and provide a brief explanation for each difference**. Please include your answers in your written submission for this assignment."],"metadata":{"id":"dVKaXiEhy1fB"}},{"cell_type":"markdown","metadata":{"id":"-JdxNRWuy1fB"},"source":["## Self-Attention: Attention by Position\n","\n","In Transformers, tokens can decide what other tokens to attend to by looking at their positions. In this section, we'll explore how this works by **hand-designing a Transformer for the task of copying the first token of a sequence across the entire sequence.**\n","\n","To accomplish this, we'll add a positional encoding to the input sequence. Transformers typically use a sinusoidal positional encoding or a learned positional encoding, but we'll **set the weight by hand to any value we choose**. These positional encodings will get concatenated to the input sequence inside the Transformer. For simplicity, we'll *concatenate* the positional encoding to the input embeddings instead of adding it.\n","\n","Here are the example data points (where `A`, `B`, and `C` are vectors and `A:pos_0` represents the concatenation between vectors `A` and `pos_0`):\n","\n","```\n","Input sequence --> Input sequence with positional encoding --> Output sequence\n","[A, B, C, C]   --> [A:pos_0, B:pos_1, C:pos_2, C:pos_3]    --> [A, A, A, A]\n","[C, A, C]      --> [C:pos_0, A:pos_1, C:pos_2]             --> [C, C, C]\n","[B, B, C]      --> [B:pos_0, B:pos_1, C:pos_2]             --> [B, B, B]\n","```\n","\n","Once you've passed the test cases, run the training loop below to train the PyTorch model.\n","\n","We have provided some hints below, but to enhance your understanding of attention and the Transformer, we highly recommend attempting this problem to the best of your abilities before referring to the hints."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vnzbWXDJy1fB","cellView":"form"},"outputs":[],"source":["#@title Hints\n","\n","# Hint 1: All hints from the previous part still apply.\n","\n","\n","# Hint 2: If you only want to use part of the information in a sequence element,\n","#         choose key/query/value matrices which remove the unwanted information.\n","seq = np.array([[1, 2, 3]])  # A sequence of length 1 with a 3-d element\n","Qm = np.array([[1, 0], [0, 0], [0, 1]])\n","print('Selecting only the first and last vector elements', seq @ Qm)\n","\n","\n","# Hint 3: You can use the following helper functions to test what keys, queries,\n","#         and values would be produced by your matrix.\n","# You will need to provide a sequence (e.g. np.stack([A, B, C])). Km, Qm, Vm, and pos are the matrices you will define below.\n","get_K = lambda seq: np.concatenate([seq, pos[:seq.shape[0]]], axis=1) @ Km # Each row of the output is a key\n","get_Q = lambda seq: np.concatenate([seq, pos[:seq.shape[0]]], axis=1) @ Qm # Each row of the output is a query\n","get_V = lambda seq: np.concatenate([seq, pos[:seq.shape[0]]], axis=1) @ Vm # Each row of the output is a value\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kls4VoqXy1fC"},"outputs":[],"source":["A = np.array([1,0,0])\n","B = np.array([0,1,0])\n","C = np.array([0,0,1])\n","\n","tokens = [A, B, C]\n","\n","################################################################################\n","# TODO: Implement numpy arrays for Km, Qm, and Vm and pos.\n","#       The shape of Km, and Qm are [input_dim + pos_dim, qk_dim].\n","#       The shape of Vm is [input_dim + pos_dim, v_dim].\n","#       The shape of pos is [max_len, pos_dim].\n","#       In this case, input_dim = 3, and v_dim = 3. qk_dim can be any value you\n","#       choose, but 1 is a reasonable choice. max_len is the maximum sequence\n","#       length you will encounter, 4 in this case.\n","#       pos_dim can be any value you choose, but 4 is a resonable choice.\n","################################################################################\n","\n","pos = NotImplementedError()\n","\n","Qm = NotImplementedError()\n","Km = NotImplementedError()\n","Vm = NotImplementedError()\n","\n","############################################ END OF YOUR CODE ##################\n","\n","\n","def generate_test_cases_first(tokens, max_len=5):\n","    seq_len = np.random.randint(1, max_len)\n","    input_arr = np.stack(random.choices(tokens, k=seq_len))\n","    # Expected output is to repeat the first row of the input k times\n","    expected_out = np.stack([input_arr[0]] * seq_len)\n","    return input_arr, expected_out\n","\n","# Test your implementation\n","show_attention = False  # Set this to True for debugging\n","for i in range(10):\n","    seq, expected_out = generate_test_cases_first(tokens)\n","    np_transformer = NumpyTransformer(Km, Qm, Vm, pos=pos)\n","    out = np_transformer.forward(seq, verbose=show_attention)\n","    if not np.allclose(out, expected_out, rtol=1e-3):\n","        print(f'FAIL: {seq} -> {out} != {expected_out}')\n","\n","_set_seed(2017)\n","seq, _ = generate_test_cases_first(tokens)\n","np_transformer = NumpyTransformer(Km, Qm, Vm, pos=pos)\n","out = np_transformer.forward(seq, verbose=show_attention)\n","TO_SAVE[\"attention_by_position\"] = out.reshape(-1).tolist()\n","TO_SAVE[\"attention_by_position_pos\"] = pos.reshape(-1).tolist()\n","TO_SAVE[\"attention_by_position_Q\"] = Qm.reshape(-1).tolist()\n","TO_SAVE[\"attention_by_position_K\"] = Km.reshape(-1).tolist()\n","TO_SAVE[\"attention_by_position_V\"] = Vm.reshape(-1).tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KRTVNgLqy1fC"},"outputs":[],"source":["# Compare the numpy and trained pytorch transformers\n","def make_batch_first(tokens=tokens, max_len=5):\n","    seq, target = generate_test_cases_first(tokens, max_len=max_len)\n","    return torch.FloatTensor(seq), torch.FloatTensor(target)\n","\n","pos_dim = pos.shape[1]\n","transformer_py, loss = train_loop(make_batch_first, input_dim=len(A), qk_dim=Km.shape[1], v_dim=Vm.shape[1], pos_dim=pos_dim, max_seq_len=pos.shape[0])\n","seq = np.stack([A, B, B])\n","out_np, out_py = compare_transformers(np_transformer, transformer_py, seq)\n","print(\"seq:\", seq)\n","print(f'Out (Hand designed) \\n {np.round(out_np, 2)}')\n","print(f' Out (Learned) \\n {np.round(out_py, 2)}')"]},{"cell_type":"markdown","source":["### Question\n","\n","In the figure provided, compare the variables of your hand-designed Transformer with those of the learned Transformer. **Identify the similarities and differences between the two sets of variables and provide a brief explanation for each.** Please include your findings in your written submission for this assignment."],"metadata":{"id":"_eKDiCuZy1fC"}},{"cell_type":"markdown","source":["## Generate the Submission Log\n","\n","Please download `submission_log.json` and submit it to Gradescope."],"metadata":{"id":"9NzDEkXwy1fC"}},{"cell_type":"code","source":["with open(\"submission_log.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(TO_SAVE, f)"],"metadata":{"id":"l4s3azIKy1fC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LQLcZYqoy1fC"},"source":["## (Optional) Self-Attention: Attention by Content and Positoin\n","\n","Finally, we'll explore how transformers can attend to tokens by looking at both their position and their content. In this section, we'll design a transformer for the following task: given a sequence of tokens, output a positive number for every unique token and a negative number for every repeated token.\n","\n","To make implementing this easier, we'll add a CLS token to the beginning of the sequence. We will ignore the output of the CLS token index, which means we can use the CLS token to represent whatever we want. (In practice, the CLS token is often thought of as a representation of the entire sequence, but you can use it however is useful.)\n","\n","\n","\\# Example data points (in each case, A, B, and C are vectors. A:pos_0 represents concatenation between vectors A and pos_0. The target outputs shown are +/-1, but any number with the right sign is fine. \"Ignore\" means that the output can be anything and will not be used to compute the loss.): \\\n","Input sequence --> Input sequence with CLS and pos encoding --> Output sequence \\\n","[A, B, C, C] --> [CLS: pos_0, A:pos_1, B:pos_2, C:pos_3, C:pos_4] --> [Ignore, 1, 1, -1, -1] \\\n","[C, A, C] --> [CLS: pos_0, C:pos_1, A:pos_2, C:pos_3] --> [Ignore, -1, 1, 1] \\\n","[B, B, C] --> [CLS: pos_0, B:pos_1, B:pos_2, C:pos_3] --> [Ignore, -1, -1, 1]\n","\n","\n","Once the test cases pass, run the training loop below a few times to train the PyTorch model. Comment on the similarities and differences between the weights and intermediate outputs of the learned and hand-coded model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_I38iqt5y1fC"},"outputs":[],"source":["A = np.array([1,0,0,0])\n","B = np.array([0,1,0,0])\n","C = np.array([0,0,1,0])\n","CLS = np.array([0,0,0,1])\n","\n","tokens = [A, B, C]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S7RYUIH0y1fC"},"outputs":[],"source":["# Hints (feel free to ignore this block if it's not useful)\n","\n","# Hint 1: All hints from the previous part still apply.\n","\n","# Hint 2: To check if an array is unique, use what you discovered in the \"select by content\" part to find rows with the same value and\n","# what you learned in the \"select by position\" part to NOT select the key which comes from the same position as the query.\n","\n","# Hint 3: If you need an offset value, consider using the CLS token The CLS token is the first token in a sequence, and it is orthogonal\n","# to all other tokens. This means you can create a query or value which selects it but not any othe token (e.g. by putting 0s in all\n","# indexes except the index where only CLS has a 1).\n","\n","# Hint 4: You can use the following helper functions to test what keys, queries, and values would be produced by your matrix.\n","# You will need to provide a sequence (e.g. np.stack([A, B, C])). Km, Qm, Vm, and pos are the matrices you will define below.\n","get_K = lambda seq: np.concatenate([np.stack([CLS] + list(seq)), pos[:seq.shape[0]+1]], axis=1) @ Km # Each row of the output is a key\n","get_Q = lambda seq: np.concatenate([np.stack([CLS] + list(seq)), pos[:seq.shape[0]+1]], axis=1) @ Qm # Each row of the output is a query\n","get_V = lambda seq: np.concatenate([np.stack([CLS] + list(seq)), pos[:seq.shape[0]+1]], axis=1) @ Vm # Each row of the output is a value\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"34S3cdv-y1fD"},"outputs":[],"source":["\n","################################################################################################\n","# TODO: Implement numpy arrays for Km, Qm, and Vm and pos.\n","#      The dimensions of Km, and Qm are (input_dim + pos_dim, qk_dim).\n","#      The dimensions of Vm are (input_dim + pos_dim, v_dim).\n","#      The dimensions of pos are (max_len + 1, pos_dim). (Each row is a position vector.)\n","#      In this case, input_dim = 4, and v_dim = 1. qk_dim can be any value you choose, but 8 is\n","#      a reasonable choice. max_len is the maximum sequence length you will encounter (before CLS is added),\n","#      4 in this case.  pos_dim can be any value you choose, but 4 is a reasonable choice.\n","#################################################################################################\n","\n","pos = NotImplementedError()\n","\n","Km = NotImplementedError()\n","\n","Qm = NotImplementedError()\n","\n","Vm = NotImplementedError()\n","\n","############################################ END OF YOUR CODE ####################################\n","\n","def generate_test_cases_unique(tokens, max_len=5):\n","    seq_len = np.random.randint(1, max_len)\n","    input_arr = np.stack(random.choices(tokens, k=seq_len))\n","    # Expected output is 1 for unique, -1 for non-unique\n","    expected_out = np.stack([1 if np.sum(np.min(input_arr == x, axis=1)) == 1 else -1 for x in input_arr]).reshape(-1, 1)\n","    # Insert CLS token as the first token in the sequence\n","    input_arr = np.stack([CLS] + list(input_arr))\n","    return input_arr, expected_out\n","\n","seq, expected_out = generate_test_cases_unique([A, B, C])\n","\n","for i in range(1):\n","    seq, expected_out = generate_test_cases_unique([A, B, C])\n","    np_transformer = NumpyTransformer(Km, Qm, Vm, pos)\n","    out = np_transformer.forward(seq, verbose=False)  # Change this to True to see the attention computation\n","    if not np.allclose(np.sign(out[1:]), expected_out, rtol=1e-3):\n","        print(f'FAIL: {seq} -> {np.sign(out[1:])} != {expected_out}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zDJNvOAhy1fD"},"outputs":[],"source":["# Compare the numpy and trained pytorch transformers\n","# Note that the pytorch transformer has a slightly harder task since it is being trained to output exactly 1 or -1, not just the sign.\n","def make_batch_unique(tokens=tokens, max_len=5):\n","    seq, target = generate_test_cases_unique(tokens, max_len=max_len)\n","    return torch.FloatTensor(seq), torch.FloatTensor(target)\n","\n","pos_dim = pos.shape[1]\n","transformer_py, loss = train_loop(make_batch_unique, input_dim=len(A), qk_dim=Km.shape[1], v_dim=Vm.shape[1], pos_dim=pos_dim, max_seq_len=pos.shape[0], remove_cls=True)\n","seq = np.stack([CLS, A, B, C, C])\n","expected_out = np.stack([1, 1, -1, -1]).reshape(-1, 1)\n","out_npy, out_pyt = compare_transformers(np_transformer, transformer_py, seq)\n","out_npy = np.sign(out_npy[1:])\n","out_pyt = np.sign(out_pyt[1:])\n","\n","# Since the CLS token is visualized above and is not part of the sequence, we remove it here.\n","# We also take the sign of the output to directly compare it to the expected output.\n","plt.figure(figsize=(10, 5))\n","plt.subplot(1, 3, 1)\n","plt.imshow(out_npy.T, vmin=-1, vmax=1)\n","plt.title('Hand-Designed Transformer')\n","plt.xticks([])\n","plt.yticks([])\n","plt.xlabel('Sequence')\n","plt.ylabel('Output')\n","plt.subplot(1, 3, 2)\n","plt.imshow(out_pyt.T, vmin=-1, vmax=1)\n","plt.title('Trained Transformer')\n","plt.xticks([])\n","plt.yticks([])\n","plt.xlabel('Sequence')\n","plt.ylabel('Output')\n","plt.subplot(1, 3, 3)\n","plt.imshow(expected_out.T, vmin=-1, vmax=1)\n","plt.title('Expected Output')\n","plt.xticks([])\n","plt.yticks([])\n","plt.xlabel('Sequence')\n","plt.ylabel('Output')\n","plt.show()\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1I5FxDDAjlvd0AlgNOmn3Jd72SXTepvPf","timestamp":1692845709050}]}},"nbformat":4,"nbformat_minor":0}