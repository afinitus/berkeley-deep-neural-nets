{"cells":[{"cell_type":"markdown","metadata":{"id":"7r94SKvgGDyH"},"source":["# Vision Transformer and Masked Autoencoder\n","\n","In this assignment, you will be implementing [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929) and [Masked Autoencoder (MAE)](https://arxiv.org/abs/2111.06377)."]},{"cell_type":"markdown","metadata":{"id":"Okuc1OksGDyI"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"jGB3w42RGDyJ"},"source":["We recommend working on Colab with GPU enabled since this assignment needs a fair amount of compute.\n","In Colab, we can enforce using GPU by clicking `Runtime -> Change Runtime Type -> Hardware accelerator` and selecting `GPU`.\n","The dependencies will be installed once the notebooks are excuted.\n","\n","You should make a copy of this notebook to your Google Drive otherwise the outputs will not be saved.\n","Once the folder is copied, you can start working by clicking a Jupyter Notebook and openning it in Colab."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lASpsjiRGDyJ"},"outputs":[],"source":["#@title Install einops\n","!python -m pip install einops"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oYYp3bk5GDyJ"},"outputs":[],"source":["#@title Import packages\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import seaborn\n","seaborn.set()\n","\n","from tqdm.notebook import trange, tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import einops\n","import pickle\n","import os\n","import io\n","import urllib.request\n","\n","torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","root_folder = colab_root_folder = os.getcwd()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9olfPSEZGDyJ"},"outputs":[],"source":["# Mount drive to save models and logs\n","# If you are not using colab, you can ignore this cell\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"28aHz-pVGDyK"},"source":["**Note**: change ```root_folder``` to the folder of this notebook in your google drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MutUd1RqGDyK"},"outputs":[],"source":["root_folder = \"/content/drive/MyDrive/cs182_hw9_mae/\"\n","os.makedirs(root_folder, exist_ok=True)\n","os.chdir(root_folder)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CNx9d-bBGDyK"},"outputs":[],"source":["#@title Download Testing Data\n","\n","def load_from_url(url):\n","    return torch.load(io.BytesIO(urllib.request.urlopen(url).read()))\n","\n","test_data = load_from_url('https://github.com/Berkeley-CS182/cs182hw9/raw/main/test_reference.pt')\n","auto_grader_data = load_from_url('https://github.com/Berkeley-CS182/cs182hw9/raw/main/autograder_student.pt')\n","auto_grader_data['output'] = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dfjq_KMjGDyK"},"outputs":[],"source":["#@title Utilities for Testing\n","def save_auto_grader_data():\n","    torch.save(\n","        {'output': auto_grader_data['output']},\n","        'autograder.pt'\n","    )\n","\n","def rel_error(x, y):\n","    return torch.max(\n","        torch.abs(x - y)\n","        / (torch.maximum(torch.tensor(1e-8), torch.abs(x) + torch.abs(y)))\n","    ).item()\n","\n","def check_error(name, x, y, tol=1e-3):\n","    error = rel_error(x, y)\n","    if error > tol:\n","        print(f'The relative error for {name} is {error}, should be smaller than {tol}')\n","    else:\n","        print(f'The relative error for {name} is {error}')\n","\n","def check_acc(acc, threshold):\n","    if acc < threshold:\n","        print(f'The accuracy {acc} should >= threshold accuracy {threshold}')\n","    else:\n","        print(f'The accuracy {acc} is better than threshold accuracy {threshold}')"]},{"cell_type":"markdown","metadata":{"id":"oAbVfnX2GDyK"},"source":["## Vision Transformer\n","The first part of this notebook is implementing Vision Transformer (ViT) and training it on CIFAR dataset.\n"]},{"cell_type":"markdown","metadata":{"id":"DYjv3Bk7GDyK"},"source":["### Image patchify and unpatchify\n","\n","In ViT, an image is split into fixed-size patches, each of them are then linearly embedded, position embeddings are added, and the resulting sequence of vectors is fed to a standard Transformer encoder. The architecture can be seen in the following figure.\n","![vit](https://github.com/google-research/vision_transformer/blob/main/vit_figure.png?raw=true)\n","\n","To get started with implementing ViT, we need to implement splitting image batch into fixed-size patches batch in ```patchify``` and combining patches batch into the original image batch in ```unpatchify```. The `patchify` function has been implemented for you. **Please implement `unpatchify`,** assuming that the output image is squared.\n","\n","This implementation uses [einops](https://github.com/arogozhnikov/einops) for flexible tensor operations, you can check out its [tutorial](https://einops.rocks/1-einops-basics/)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8gguN2QGDyK"},"outputs":[],"source":["def patchify(images, patch_size=4):\n","    \"\"\"Splitting images into patches.\n","    Args:\n","        images: Input tensor with size (batch, channels, height, width)\n","    Returns:\n","        A batch of image patches with size (\n","          batch, (height / patch_size) * (width / patch_size),\n","        channels * patch_size * patch_size)\n","\n","    Hint: use einops.rearrange. The \"space-to-depth operation\" example at https://einops.rocks/api/rearrange/\n","    is not exactly what you need, but it gives a good idea of how to use rearrange.\n","    \"\"\"\n","    return einops.rearrange(\n","        images,\n","        'b c (h p1) (w p2) -> b (h w) (c p1 p2)',\n","        p1=patch_size,\n","        p2=patch_size\n","    )\n","\n","def unpatchify(patches, patch_size=4):\n","    \"\"\"Combining patches into images.\n","    Args:\n","        patches: Input tensor with size (\n","        batch, (height / patch_size) * (width / patch_size),\n","        channels * patch_size * patch_size)\n","    Returns:\n","        A batch of images with size (batch, channels, height, width)\n","\n","    Hint: einops.rearrange can be used here as well.\n","    Hint: we assume that the output image is squared, so you can feel free to\n","          use `sqrt` to calculate image dimensions.\n","    \"\"\"\n","    ############################################################################\n","    # TODO: implement this function\n","    ############################################################################\n","    raise NotImplementedError()\n","    ############################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"sEu0weTEGDyK"},"outputs":[],"source":["#@title Test your implementation\n","x = test_data['input']['patchify']\n","y = test_data['output']['patchify']\n","check_error('patchify', patchify(x), y)\n","\n","x = auto_grader_data['input']['patchify']\n","auto_grader_data['output']['patchify'] = patchify(x)\n","save_auto_grader_data()\n","\n","\n","x = test_data['input']['unpatchify']\n","y = test_data['output']['unpatchify']\n","check_error('unpatchify', unpatchify(x), y)\n","\n","x = auto_grader_data['input']['unpatchify']\n","auto_grader_data['output']['unpatchify'] = unpatchify(x)\n","\n","save_auto_grader_data()"]},{"cell_type":"markdown","metadata":{"id":"kYyNvF1GGDyL"},"source":["### ViT Model\n","\n","Here is an implementation of a Transformer. It simply wraps `nn.TransformerEncoder` of PyTorch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CzYsjGPIGDyL"},"outputs":[],"source":["class Transformer(nn.Module):\n","    \"\"\"Transformer Encoder\n","    Args:\n","        embedding_dim: dimension of embedding\n","        n_heads: number of attention heads\n","        n_layers: number of attention layers\n","        feedforward_dim: hidden dimension of MLP layer\n","    Returns:\n","        Transformer embedding of input\n","    \"\"\"\n","    def __init__(self, embedding_dim=256, n_heads=4, n_layers=4, feedforward_dim=1024):\n","        super().__init__()\n","        self.embedding_dim = embedding_dim\n","        self.n_layers = n_layers\n","        self.n_heads = n_heads\n","        self.feedforward_dim = feedforward_dim\n","        self.transformer = nn.TransformerEncoder(\n","            nn.TransformerEncoderLayer(\n","                d_model=embedding_dim,\n","                nhead=self.n_heads,\n","                dim_feedforward=self.feedforward_dim,\n","                activation=F.gelu,\n","                batch_first=True,\n","                dropout=0.0,\n","            ),\n","            num_layers=n_layers,\n","        )\n","\n","    def forward(self, x):\n","        return self.transformer(x)"]},{"cell_type":"markdown","source":["**Implement the `forward` method of `ClassificationViT`**, use the layers defined in the constructor and `patchify`/`unpachify` function implemented above."],"metadata":{"id":"kggZXHZrGDyL"}},{"cell_type":"code","source":["class ClassificationViT(nn.Module):\n","    \"\"\"Vision transformer for classfication\n","    Args:\n","        n_classes: number of classes\n","        embedding_dim: dimension of embedding\n","        patch_size: image patch size\n","        num_patches: number of image patches\n","    Returns:\n","        Logits of classfication\n","    \"\"\"\n","    def __init__(self, n_classes, embedding_dim=256, patch_size=4, num_patches=8):\n","        super().__init__()\n","        self.patch_size = patch_size\n","        self.num_patches = num_patches\n","        self.embedding_dim = embedding_dim\n","\n","        self.transformer = Transformer(embedding_dim)\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim) * 0.02)\n","        self.position_encoding = nn.Parameter(\n","            torch.randn(1, num_patches * num_patches + 1, embedding_dim) * 0.02\n","        )\n","        self.patch_projection = nn.Linear(patch_size * patch_size * 3, embedding_dim)\n","\n","        # A Layernorm and a Linear layer are applied on ViT encoder embeddings\n","        self.output_head = nn.Sequential(\n","            nn.LayerNorm(embedding_dim), nn.Linear(embedding_dim, n_classes)\n","        )\n","\n","    def forward(self, images):\n","        \"\"\"\n","        (1) Splitting images into fixed-size patches;\n","        (2) Linearly embed each image patch, prepend CLS token;\n","        (3) Add position embeddings;\n","        (4) Feed the resulting sequence of vectors to Transformer encoder.\n","        (5) Extract the embeddings corresponding to the CLS token.\n","        (6) Apply output head to the embeddings to obtain the logits\n","        \"\"\"\n","        ########################################################################\n","        # TODO: implement this function\n","        ########################################################################\n","        raise NotImplementedError()\n","        ########################################################################"],"metadata":{"id":"LzzqYkZEGDyL"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"YREml62zGDyL"},"outputs":[],"source":["#@title Test your implementation\n","model = ClassificationViT(10)\n","model.load_state_dict(test_data['weights']['ClassificationViT'])\n","x = test_data['input']['ClassificationViT.forward']\n","y = model.forward(x)\n","check_error('ClassificationViT.forward', y, test_data['output']['ClassificationViT.forward'])\n","\n","model.load_state_dict(auto_grader_data['weights']['ClassificationViT'])\n","x = auto_grader_data['input']['ClassificationViT.forward']\n","y = model.forward(x)\n","auto_grader_data['output']['ClassificationViT.forward'] = y\n","save_auto_grader_data()"]},{"cell_type":"markdown","metadata":{"id":"DPGS_5jwGDyL"},"source":["### Data Loader and Preprocess\n","\n","We use ```torchvision``` to download and prepare images and labels. ViT usually works on a much larger image dataset, but due to our limited computational resources, we train our ViT on CIFAR-10."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cMDpMAT7GDyL"},"outputs":[],"source":["transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.Resize(32),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.Resize(32),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","batch_size = 128\n","\n","trainset = torchvision.datasets.CIFAR10(\n","    root=os.path.join(colab_root_folder, 'data'),\n","    train=True, download=True, transform=transform_train\n",")\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(\n","    root=os.path.join(colab_root_folder, 'data'),\n","    train=False, download=True, transform=transform_test\n",")\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=False, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"AAkdejtOGDyL"},"source":["### Supervised Training ViT\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UpAyNbZpGDyL"},"outputs":[],"source":["# Initilize model (ClassificationViT)\n","model = ClassificationViT(10)\n","# Move model to GPU\n","model.to(torch_device)\n","# Create optimizer for the model\n","\n","# You may want to tune these hyperparameters to get better performance\n","optimizer = optim.AdamW(model.parameters(), lr=1e-3, betas=(0.9, 0.95), weight_decay=1e-9)\n","\n","total_steps = 0\n","num_epochs = 10\n","train_logfreq = 100\n","losses = []\n","train_acc = []\n","all_val_acc = []\n","best_val_acc = 0\n","\n","epoch_iterator = trange(num_epochs)\n","for epoch in epoch_iterator:\n","    # Train\n","    data_iterator = tqdm(trainloader)\n","    for x, y in data_iterator:\n","        total_steps += 1\n","        x, y = x.to(torch_device), y.to(torch_device)\n","        logits = model(x)\n","        loss = torch.mean(F.cross_entropy(logits, y))\n","        accuracy = torch.mean((torch.argmax(logits, dim=-1) == y).float())\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        data_iterator.set_postfix(loss=loss.item(), train_acc=accuracy.item())\n","\n","        if total_steps % train_logfreq == 0:\n","            losses.append(loss.item())\n","            train_acc.append(accuracy.item())\n","\n","    # Validation\n","    val_acc = []\n","    model.eval()\n","    for x, y in testloader:\n","        x, y = x.to(torch_device), y.to(torch_device)\n","        with torch.no_grad():\n","          logits = model(x)\n","        accuracy = torch.mean((torch.argmax(logits, dim=-1) == y).float())\n","        val_acc.append(accuracy.item())\n","    model.train()\n","\n","    all_val_acc.append(np.mean(val_acc))\n","    # Save best model\n","    if np.mean(val_acc) > best_val_acc:\n","        best_val_acc = np.mean(val_acc)\n","\n","    epoch_iterator.set_postfix(val_acc=np.mean(val_acc), best_val_acc=best_val_acc)\n","\n","plt.plot(losses)\n","plt.title('Train Loss')\n","plt.figure()\n","plt.plot(train_acc)\n","plt.title('Train Accuracy')\n","plt.figure()\n","plt.plot(all_val_acc)\n","plt.title('Val Accuracy')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"a6mkwWw3GDyL"},"outputs":[],"source":["#@title Test your implementation\n","auto_grader_data['output']['vit_acc'] = best_val_acc\n","save_auto_grader_data()\n","check_acc(best_val_acc, threshold=0.65)"]},{"cell_type":"markdown","metadata":{"id":"TqQ5xYisGDyL"},"source":["## Masked AutoEncoder\n","\n","The second part of this notebook is implementing Masked Autoencoder (MAE).\n","The idea of MAE is masking random patches of the input image and reconstruct the missing pixels. This whole achitecture can be seen in the following figure.\n","![mae](https://user-images.githubusercontent.com/11435359/146857310-f258c86c-fde6-48e8-9cee-badd2b21bd2c.png)\n","\n","You will train MAE without labels on CIFAR, aka, self-supervised learning.\n","Then you will use the self-supervised pretrained model for linear classification and finetuning experiments."]},{"cell_type":"markdown","metadata":{"id":"uKkZJgSAGDyL"},"source":["### Random Masking and Restore\n","\n","**Implement ```random_masking``` to mask random patches from the input image and ```restore_masked``` to combine reconstructed masked part and unmasked part to restore the image.**\n","\n","The `index_sequence` utility function has been provided to you, along with two examples:"]},{"cell_type":"code","source":["def index_sequence(x, ids):\n","    \"\"\"Index tensor (x) with indices given by ids\n","    Args:\n","        x: input sequence tensor, can be 2D (batch x length) or 3D (batch x length x feature)\n","        ids: 2D indices (batch x length) for re-indexing the sequence tensor\n","    \"\"\"\n","    if len(x.shape) == 3:\n","        ids = ids.unsqueeze(-1).expand(-1, -1, x.shape[-1])\n","    return torch.take_along_dim(x, ids, dim=1)"],"metadata":{"id":"R6muXjTcGDyM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(index_sequence(\n","    torch.tensor([\n","        [0.0, 0.1, 0.2],\n","        [1.0, 1.1, 1.2]\n","    ], dtype=torch.float),\n","    torch.tensor([\n","        [0, 2],\n","        [0, 1]\n","    ], dtype=torch.long)\n","))"],"metadata":{"id":"H56onDvQGDyM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(index_sequence(\n","    torch.tensor([\n","        [[0.01, 0.02], [0.11, 0.12], [0.21, 0.22]],\n","        [[1.01, 1.02], [1.11, 1.12], [1.21, 1.22]]\n","    ], dtype=torch.float),\n","    torch.tensor([\n","        [0, 2],\n","        [0, 1]\n","    ], dtype=torch.long)\n","))"],"metadata":{"id":"bu_TryRbGDyM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"84o5KKOcGDyM"},"outputs":[],"source":["def random_masking(x, keep_length, ids_shuffle):\n","    \"\"\"Apply random masking on input tensor\n","    Args:\n","        x: input patches (batch x length x feature)\n","        keep_length: length of unmasked patches\n","        ids_shuffle: random indices for shuffling the input sequence. This is an\n","            array of size (batch x length) where each row is a permutation of\n","            [0, 1, ..., length-1]. We will pass this array to index_sequence function\n","            to chooose the unmasked patches.\n","\n","    Returns:\n","        kept: unmasked part of x: (batch x keep_length x feature)\n","        mask: a 2D (batch x length) mask tensor of 0s and 1s indicated which\n","            part of x is masked out. The value 0 indicates not masked and 1\n","            indicates masked.\n","        ids_restore: indices to restore x. This is an array of size (batch x length).\n","            If we take the kept part and masked\n","            part of x, concatentate them together and index it with ids_restore,\n","            we should get x back. (Hint: try using torch.argsort on the shuffle indices)\n","    \"\"\"\n","    ############################################################################\n","    # TODO: implement this function\n","    ############################################################################\n","    raise NotImplementedError()\n","    ############################################################################\n","\n","def restore_masked(kept_x, masked_x, ids_restore):\n","    \"\"\"Restore masked patches\n","    Args:\n","        kept_x: unmasked patches: (batch x keep_length x feature)\n","        masked_x: masked patches: (batch x (length - keep_length) x feature)\n","        ids_restore: indices to restore x: (batch x length)\n","    Returns:\n","        restored patches\n","    Hint: use index_sequence function on an array with the kept and masked tokens concatenated\n","    \"\"\"\n","    ############################################################################\n","    # TODO: implement this function\n","    ############################################################################\n","    raise NotImplementedError()\n","    ############################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"j0LeUViqGDyM"},"outputs":[],"source":["#@title Test your implementation\n","x, ids_shuffle = test_data['input']['random_masking']\n","kept, mask, ids_restore = random_masking(x, 4, ids_shuffle)\n","kept_t, mask_t, ids_restore_t = test_data['output']['random_masking']\n","check_error('random_masking: kept', kept, kept_t)\n","check_error('random_masking: mask', mask, mask_t)\n","check_error('random_masking: ids_restore', ids_restore, ids_restore_t)\n","\n","x, ids_shuffle = auto_grader_data['input']['random_masking']\n","kept, mask, ids_restore = random_masking(x, 4, ids_shuffle)\n","auto_grader_data['output']['random_masking'] = (kept, mask, ids_restore)\n","save_auto_grader_data()\n","\n","kept_x, masked_x, ids_restore = test_data['input']['restore_masked']\n","restored = restore_masked(kept_x, masked_x, ids_restore)\n","check_error('restore_masked', restored, test_data['output']['restore_masked'])\n","\n","kept_x, masked_x, ids_restore = auto_grader_data['input']['restore_masked']\n","restored = restore_masked(kept_x, masked_x, ids_restore)\n","auto_grader_data['output']['restore_masked'] = (kept, mask, ids_restore)\n","save_auto_grader_data()"]},{"cell_type":"markdown","source":["### Masked Autoencoder\n","\n","**Implement the following methods of `MaskedAutoEncoder`**:\n","\n","- `forward_encoder`: Encodes the input images. It involves patchifying images into patches, randomly masking some patches, and encode the masked image with the ViT encoder. The mask information should also be returned, which will then be passed to the `forward` method.\n","\n","- `forward_decoder`: Decodes the encoder embeddings. It involves restoring the sequence from masked patches and encoder predictions using ViT decoder, and projecting to predict image patches.\n","\n","- `forward_encoder_representation`: Encodes images without applying random masking to get a representation of the input images."],"metadata":{"id":"CTFXxVVKGDyM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"C_PxG7gIGDyM"},"outputs":[],"source":["class MaskedAutoEncoder(nn.Module):\n","    \"\"\"MAE Encoder\n","    Args:\n","        encoder: vit encoder\n","        decoder: vit decoder\n","        encoder_embedding_dim: embedding size of encoder\n","        decoder_embedding_dim: embedding size of decoder\n","        patch_size: image patch size\n","        num_patches: number of patches\n","        mask_ratio: percentage of masked patches\n","    \"\"\"\n","    def __init__(self, encoder, decoder, encoder_embedding_dim=256,\n","                 decoder_embedding_dim=128, patch_size=4, num_patches=8,\n","                 mask_ratio=0.75):\n","        super().__init__()\n","        self.encoder_embedding_dim = encoder_embedding_dim\n","        self.decoder_embedding_dim = decoder_embedding_dim\n","        self.patch_size = patch_size\n","        self.num_patches = num_patches\n","        self.mask_ratio = mask_ratio\n","\n","        self.masked_length = int(num_patches * num_patches * mask_ratio)\n","        self.keep_length = num_patches * num_patches - self.masked_length\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","        self.encoder_input_projection = nn.Linear(patch_size * patch_size * 3, encoder_embedding_dim)\n","        self.decoder_input_projection = nn.Linear(encoder_embedding_dim, decoder_embedding_dim)\n","        self.decoder_output_projection = nn.Linear(decoder_embedding_dim, patch_size * patch_size * 3)\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, encoder_embedding_dim) * 0.02)\n","        self.encoder_position_encoding = nn.Parameter(torch.randn(1, num_patches * num_patches, encoder_embedding_dim) * 0.02)\n","        self.decoder_position_encoding = nn.Parameter(torch.randn(1, num_patches * num_patches, decoder_embedding_dim) * 0.02)\n","        self.masked_tokens = nn.Parameter(torch.randn(1, 1, decoder_embedding_dim) * 0.02)\n","\n","    def forward_encoder(self, images, ids_shuffle=None):\n","        \"\"\"\n","        Encode input images using the following steps:\n","\n","        1. Divide the images into smaller patches using the `patchify` function.\n","        2. Apply a linear projection to each image patch.\n","        3. Add position encoding to the projected patches.\n","        4. Mask out a subset of patches using the `random_masking` function.\n","           - Note that `ids_shuffle` is optional. If it is omitted, you need to\n","             generate a random permutation of patch indices and pass it to the\n","             `random_masking` function\n","        5. Concatenate the CLS token embedding with the masked patch embeddings.\n","           - The embedding of the CLS token is defined as `self.cls_token`\n","        6. Pass the combined tensor to the ViT encoder and return its output,\n","           along with the mask and the ids_restore tensor obtained in step 4.\n","        \"\"\"\n","        ########################################################################\n","        # TODO: implement this function\n","        ########################################################################\n","        raise NotImplementedError()\n","        ########################################################################\n","\n","    def forward_decoder(self, encoder_embeddings, ids_restore):\n","        \"\"\"\n","        Decode encoder embeddings using the following steps:\n","\n","        1. Apply a linear projection to the encoder output.\n","        2. Extract the CLS token from the projected decoder embeddings and set\n","           it aside.\n","        3. Restore the sequence by inserting MASK tokens into the decoder\n","           embeddings, while also removing the CLS token from the sequence.\n","           - The embedding of the MASK token is defined as `self.masked_tokens`\n","        4. Add position encoding to the restored decoder embeddings.\n","        5. Re-concatenate the CLS token with the decoder embeddings.\n","        6. Pass the combined tensor to the ViT decoder, and retrieve the decoder\n","           output by excluding the CLS token.\n","        7. Apply the decoder output projection to the decoder output to predict\n","           image patches, and return the result.\n","        \"\"\"\n","        ########################################################################\n","        # TODO: implement this function\n","        ########################################################################\n","        raise NotImplementedError()\n","        ########################################################################\n","\n","    def forward(self, images):\n","        encoder_output, mask, ids_restore = self.forward_encoder(images)\n","        decoder_output = self.forward_decoder(encoder_output, ids_restore)\n","        return decoder_output, mask\n","\n","    def forward_encoder_representation(self, images):\n","        \"\"\"\n","        Encode input images **without** applying random masking, following step\n","        1, 2, 3, 5, 6 of `forward_encoder`\n","        \"\"\"\n","        ########################################################################\n","        # TODO: implement this function\n","        ########################################################################\n","        raise NotImplementedError()\n","        ########################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Jdegs70fGDyM"},"outputs":[],"source":["#@title Test your implementation\n","model = MaskedAutoEncoder(\n","    Transformer(embedding_dim=256, n_layers=4),\n","    Transformer(embedding_dim=128, n_layers=2),\n",")\n","\n","model.load_state_dict(test_data['weights']['MaskedAutoEncoder'])\n","images, ids_shuffle = test_data['input']['MaskedAutoEncoder.forward_encoder']\n","encoder_embeddings_t, mask_t, ids_restore_t = test_data['output']['MaskedAutoEncoder.forward_encoder']\n","encoder_embeddings, mask, ids_restore = model.forward_encoder(\n","    images, ids_shuffle\n",")\n","\n","check_error(\n","    'MaskedAutoEncoder.forward_encoder: encoder_embeddings',\n","    encoder_embeddings, encoder_embeddings_t, .008\n",")\n","check_error(\n","    'MaskedAutoEncoder.forward_encoder: mask',\n","    mask, mask_t\n",")\n","check_error(\n","    'MaskedAutoEncoder.forward_encoder: ids_restore',\n","    ids_restore, ids_restore_t\n",")\n","\n","encoder_embeddings, ids_restore = test_data['input']['MaskedAutoEncoder.forward_decoder']\n","decoder_output_t = test_data['output']['MaskedAutoEncoder.forward_decoder']\n","decoder_output = model.forward_decoder(encoder_embeddings, ids_restore)\n","check_error(\n","    'MaskedAutoEncoder.forward_decoder',\n","    decoder_output,\n","    decoder_output_t, .03\n",")\n","\n","images = test_data['input']['MaskedAutoEncoder.forward_encoder_representation']\n","encoder_representations_t = test_data['output']['MaskedAutoEncoder.forward_encoder_representation']\n","encoder_representations = model.forward_encoder_representation(images)\n","check_error(\n","    'MaskedAutoEncoder.forward_encoder_representation',\n","    encoder_representations,\n","    encoder_representations_t, .01\n",")\n","\n","\n","\n","model = MaskedAutoEncoder(\n","    Transformer(embedding_dim=256, n_layers=4),\n","    Transformer(embedding_dim=128, n_layers=2),\n",")\n","\n","model.load_state_dict(auto_grader_data['weights']['MaskedAutoEncoder'])\n","images, ids_shuffle = auto_grader_data['input']['MaskedAutoEncoder.forward_encoder']\n","auto_grader_data['output']['MaskedAutoEncoder.forward_encoder'] = model.forward_encoder(\n","    images, ids_shuffle\n",")\n","\n","encoder_embeddings, ids_restore = auto_grader_data['input']['MaskedAutoEncoder.forward_decoder']\n","auto_grader_data['output']['MaskedAutoEncoder.forward_decoder'] = model.forward_decoder(encoder_embeddings, ids_restore)\n","\n","images = auto_grader_data['input']['MaskedAutoEncoder.forward_encoder_representation']\n","auto_grader_data['output']['MaskedAutoEncoder.forward_encoder_representation'] = model.forward_encoder_representation(images)\n","save_auto_grader_data()\n"]},{"cell_type":"markdown","metadata":{"id":"qXE-8D3jGDyM"},"source":["### Train Masked Autoencoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K3Yn0gcMGDyM"},"outputs":[],"source":["# Initilize MAE model\n","model = MaskedAutoEncoder(\n","    Transformer(embedding_dim=256, n_layers=4),\n","    Transformer(embedding_dim=128, n_layers=2),\n",")\n","# Move the model to GPU\n","model.to(torch_device)\n","# Create optimizer\n","\n","# You may want to tune these hyperparameters to get better performance\n","optimizer = optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.95), weight_decay=0.05)\n","\n","total_steps = 0\n","num_epochs = 20\n","train_logfreq = 100\n","\n","losses = []\n","\n","epoch_iterator = trange(num_epochs)\n","for epoch in epoch_iterator:\n","    # Train\n","    data_iterator = tqdm(trainloader)\n","    for x, y in data_iterator:\n","        total_steps += 1\n","        x = x.to(torch_device)\n","        image_patches = patchify(x)\n","        predicted_patches, mask = model(x)\n","        loss = torch.sum(torch.mean(torch.square(image_patches - predicted_patches), dim=-1) * mask) / mask.sum()\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        data_iterator.set_postfix(loss=loss.item())\n","        if total_steps % train_logfreq == 0:\n","            losses.append(loss.item())\n","\n","    # Periodically save model\n","    torch.save(model.state_dict(), os.path.join(root_folder, \"mae_pretrained.pt\"))\n","\n","plt.plot(losses)\n","plt.title('MAE Train Loss')"]},{"cell_type":"markdown","metadata":{"id":"Fb_WEjb7GDyM"},"source":["### Use pretrained MAE model for classification\n","\n","As ViT has a class token, to adapt to this design, in our MAE pre-training we append an auxiliary dummy token to the encoder input. This token will be treated as the class token for training the classifier in linear probing and fine-tuning.\n","\n","The `ClassificationMAE` class wraps your pretrained MAE and leverage the CLS token for classification. **Implement the `forward` method of `ClassificationMAE`.** It should support two modes controlled by the `detach` flag:\n","\n","- Linear probe mode (`detach` is true): the backpropagation does not run through the pretrained MAE backbone, and only the output classification layer is updated during training.\n","\n","- Full finetuning mode (`detach` is false): the MAE backbone as well as the classification layer is updated during training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NP8iplS5GDyM"},"outputs":[],"source":["class ClassificationMAE(nn.Module):\n","    \"\"\"A linear classifier is trained on self-supervised representations learned by MAE.\n","    Args:\n","        n_classes: number of classes\n","        mae: mae model\n","        embedding_dim: embedding dimension of mae output\n","        detach: if True, only the classification head is updated.\n","    \"\"\"\n","    def __init__(self, n_classes, mae, embedding_dim=256, detach=False):\n","        super().__init__()\n","        self.embedding_dim = embedding_dim\n","        self.mae = mae\n","        self.output_head = nn.Sequential(\n","            nn.LayerNorm(embedding_dim), nn.Linear(embedding_dim, n_classes)\n","        )\n","        self.detach = detach\n","\n","    def forward(self, images):\n","        \"\"\"\n","        Args:\n","            Images: batch of images\n","        Returns:\n","            logits: batch of logits from the ouput_head\n","        Remember to detach the representations if self.detach=True, and\n","        Remember that we do not use masking here.\n","        \"\"\"\n","        ########################################################################\n","        # TODO: implement this function\n","        ########################################################################\n","        raise NotImplementedError()\n","        ########################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"jkMzZ1DUGDyM"},"outputs":[],"source":["#@title Test your implementation\n","model = ClassificationMAE(\n","    10,\n","    MaskedAutoEncoder(\n","        Transformer(embedding_dim=256, n_layers=4),\n","        Transformer(embedding_dim=128, n_layers=2),\n","    )\n",")\n","\n","model.load_state_dict(test_data['weights']['ClassificationMAE'])\n","\n","check_error(\n","    'ClassificationMAE.forward',\n","    model(test_data['input']['ClassificationMAE.forward']),\n","    test_data['output']['ClassificationMAE.forward']\n",")\n","\n","model = ClassificationMAE(\n","    10,\n","    MaskedAutoEncoder(\n","        Transformer(embedding_dim=256, n_layers=4),\n","        Transformer(embedding_dim=128, n_layers=2),\n","    )\n",")\n","\n","model.load_state_dict(auto_grader_data['weights']['ClassificationMAE'])\n","auto_grader_data['output']['ClassificationMAE.forward'] = model(\n","    auto_grader_data['input']['ClassificationMAE.forward']\n",")\n","save_auto_grader_data()"]},{"cell_type":"markdown","metadata":{"id":"GzgUtAaPGDyN"},"source":["### Load the pretrained MAE model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cr7psYRjGDyN"},"outputs":[],"source":["mae = MaskedAutoEncoder(\n","    Transformer(embedding_dim=256, n_layers=4),\n","    Transformer(embedding_dim=128, n_layers=2),\n",")\n","mae.load_state_dict(torch.load(os.path.join(root_folder, \"mae_pretrained.pt\")))"]},{"cell_type":"markdown","metadata":{"id":"Tg_zLMxEGDyN"},"source":["### Linear Classification\n","\n","A linear classifier is trained on self-supervised representations learned by MAE."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RlqT_UYqGDyN"},"outputs":[],"source":["# Initilize classification model; set detach=True to only update the linear classifier.\n","model = ClassificationMAE(10, mae, detach=True)\n","model.to(torch_device)\n","\n","# You may want to tune these hyperparameters to get better performance\n","optimizer = optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.95), weight_decay=1e-9)\n","\n","total_steps = 0\n","num_epochs = 20\n","train_logfreq = 100\n","losses = []\n","train_acc = []\n","all_val_acc = []\n","best_val_acc = 0\n","\n","epoch_iterator = trange(num_epochs)\n","for epoch in epoch_iterator:\n","    # Train\n","    data_iterator = tqdm(trainloader)\n","    for x, y in data_iterator:\n","        total_steps += 1\n","        x, y = x.to(torch_device), y.to(torch_device)\n","        logits = model(x)\n","        loss = torch.mean(F.cross_entropy(logits, y))\n","        accuracy = torch.mean((torch.argmax(logits, dim=-1) == y).float())\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        data_iterator.set_postfix(loss=loss.item(), train_acc=accuracy.item())\n","\n","        if total_steps % train_logfreq == 0:\n","            losses.append(loss.item())\n","            train_acc.append(accuracy.item())\n","\n","    # Validation\n","    val_acc = []\n","    model.eval()\n","    for x, y in testloader:\n","        x, y = x.to(torch_device), y.to(torch_device)\n","        with torch.no_grad():\n","          logits = model(x)\n","        accuracy = torch.mean((torch.argmax(logits, dim=-1) == y).float())\n","        val_acc.append(accuracy.item())\n","\n","    model.train()\n","\n","    all_val_acc.append(np.mean(val_acc))\n","\n","    # Save best model\n","    if np.mean(val_acc) > best_val_acc:\n","        best_val_acc = np.mean(val_acc)\n","\n","    epoch_iterator.set_postfix(val_acc=np.mean(val_acc), best_val_acc=best_val_acc)\n","\n","plt.plot(losses)\n","plt.title('Linear Classification Train Loss')\n","plt.figure()\n","plt.plot(train_acc)\n","plt.title('Linear Classification Train Accuracy')\n","plt.figure()\n","plt.plot(all_val_acc)\n","plt.title('Linear Classification Val Accuracy')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"487TRFOZGDyN"},"outputs":[],"source":["#@title Test your implementation\n","auto_grader_data['output']['mae_linear_acc'] = best_val_acc\n","save_auto_grader_data()\n","check_acc(best_val_acc, threshold=0.30)"]},{"cell_type":"markdown","metadata":{"id":"eGWlV7wWGDyN"},"source":["### Full Finetuning\n","\n","A linear classifer and the pretrained MAE model are jointly updated."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nlknV4oCGDyN"},"outputs":[],"source":["# Initilize classification model; set detach=False to update both the linear classifier and pretrained MAE model.\n","model = ClassificationMAE(10, mae, detach=False)\n","model.to(torch_device)\n","\n","# You may want to tune these hyperparameters to get better performance\n","optimizer = optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.95), weight_decay=1e-9)\n","\n","total_steps = 0\n","num_epochs = 20\n","train_logfreq = 100\n","losses = []\n","train_acc = []\n","all_val_acc = []\n","best_val_acc = 0\n","\n","epoch_iterator = trange(num_epochs)\n","for epoch in epoch_iterator:\n","    # Train\n","    data_iterator = tqdm(trainloader)\n","    for x, y in data_iterator:\n","        total_steps += 1\n","        x, y = x.to(torch_device), y.to(torch_device)\n","        logits = model(x)\n","        loss = torch.mean(F.cross_entropy(logits, y))\n","        accuracy = torch.mean((torch.argmax(logits, dim=-1) == y).float())\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        data_iterator.set_postfix(loss=loss.item(), train_acc=accuracy.item())\n","\n","        if total_steps % train_logfreq == 0:\n","            losses.append(loss.item())\n","            train_acc.append(accuracy.item())\n","\n","    # Validation\n","    val_acc = []\n","    model.eval()\n","    for x, y in testloader:\n","        x, y = x.to(torch_device), y.to(torch_device)\n","        with torch.no_grad():\n","          logits = model(x)\n","        accuracy = torch.mean((torch.argmax(logits, dim=-1) == y).float())\n","        val_acc.append(accuracy.item())\n","    model.train()\n","\n","    all_val_acc.append(np.mean(val_acc))\n","\n","    # Save best model\n","    if np.mean(val_acc) > best_val_acc:\n","        best_val_acc = np.mean(val_acc)\n","\n","    epoch_iterator.set_postfix(val_acc=np.mean(val_acc), best_val_acc=best_val_acc)\n","\n","plt.plot(losses)\n","plt.title('Finetune Classification Train Loss')\n","plt.figure()\n","plt.plot(train_acc)\n","plt.title('Finetune Classification Train Accuracy')\n","plt.figure()\n","plt.plot(all_val_acc)\n","plt.title('Finetune Classification Val Accuracy')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mv374X-vGDyN"},"outputs":[],"source":["#@title Test your implementation\n","auto_grader_data['output']['mae_finetune_acc'] = best_val_acc\n","save_auto_grader_data()\n","check_acc(best_val_acc, threshold=0.70)"]},{"cell_type":"markdown","metadata":{"id":"djClwjZ_GDyN"},"source":["## Prepare Gradescope submission\n","\n","**NOTE:** change the following path to your ```root_dir``` in the begining.\n","\n","Run the following cell will automatically prepare and download ```hw9_submission.zip```.\n","\n","Upload the downloaded file to Gradescope.\n","The Gradescope will run an autograder on the files you submit.\n","\n","It is very unlikely but still possible that your implementation might fail to pass some test cases due to randomness.\n","If you think your code is correct, you can simply rerun the autograder to check check whether it is really due to randomness."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y7AvJK7NGDyN"},"outputs":[],"source":["%cd /content/drive/MyDrive/cs182_hw9_mae\n","!pwd # make sure we are in the right dir\n","\n","!rm hw9_submission.zip\n","!zip hw9_submission.zip -r *.ipynb autograder.pt\n","\n","from google.colab import files\n","files.download('hw9_submission.zip')"]}],"metadata":{"colab":{"provenance":[{"file_id":"1A6_pAFcNEg3Scz3_rnN-qljmekNk0qCd","timestamp":1692845713347}]}},"nbformat":4,"nbformat_minor":0}